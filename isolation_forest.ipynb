{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfd1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9c8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data, in this case data are tabluar and of mixed types\n",
    "kaggle_set = pd.read_csv('/Users/sara/Desktop/train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "# Authenticate with the Kaggle API using your credentials\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "    # Download the dataset\n",
    "    # Replace 'username/dataset-name' with the actual dataset identifier\n",
    "    # The 'path' parameter specifies where to save the downloaded files\n",
    "    # 'unzip=True' will automatically extract the contents if it's a zip file\n",
    "kaggle.api.dataset_download_files('username/dataset-name', path='./data', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0811b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the proportion of fraud cases so we can set contamination level\n",
    "fraud_proportion = kaggle_set['isFraud'].sum()/kaggle_set.shape[0]  \n",
    "\n",
    "# Drop the labels so they can't be used as a factor\n",
    "df = kaggle_set.copy().drop(columns=['isFraud']) \n",
    "\n",
    "# Add a count of nans in columns. Not using one-hot encoding because it will cluster fraud cases together. \n",
    "df['nan_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# Separate into categorical and numerical factors\n",
    "categorical_df = df.select_dtypes(include=['object']).copy()\n",
    "#fill missing values in categorical columns with the mode of the column\n",
    "for column in categorical_df.columns:\n",
    "    if categorical_df[column].isnull().any(): \n",
    "        categorical_df[column] = categorical_df[column].fillna(categorical_df[column].mode()[0])\n",
    "        \n",
    "\n",
    "categorical_df = categorical_df.apply(lambda x: LabelEncoder().fit_transform(x.astype(str)))\n",
    "\n",
    "numerical_df = df.select_dtypes(exclude=['object']).copy()\n",
    "# Fill missing values in numerical columns with the mean of the column\n",
    "for column in numerical_df.columns:\n",
    "    # Convert numerical columns to float type and Z-normalize\n",
    "    numerical_df[column] = numerical_df[column].astype(float)\n",
    "    numerical_df[column] = (numerical_df[column]- numerical_df[column].mean())/ numerical_df[column].std()\n",
    "\n",
    "    # Fill missing values with the mean of the column (0 after normalization)\n",
    "    numerical_df[column] = numerical_df[column].fillna(numerical_df[column].mean())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea71e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # Keep 79% of variance (I think this was Adrian's number but correct me if I'm wrong)\n",
    "our_pca = pca.fit_transform(numerical_df.drop(columns=['nan_count']) )\n",
    "\n",
    "df_pca = pd.DataFrame(our_pca, columns=[f'PC{i+1}' for i in range(our_pca.shape[1])])\n",
    "# add the categorical and nancount columns back to the PCA dataframe\n",
    "df_pca = pd.concat([df_pca, categorical_df.reset_index(drop=True)], axis = 1)\n",
    "df_pca['nan_count'] = numerical_df['nan_count'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(data, contamination, random_state=42):\n",
    "#takes a DataFrame, applies Isolation Forest, and returns a DataFrame with anomaly scores and predictions.\n",
    "\n",
    "    # Step 4: Isolation Forest\n",
    "    model = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    model.fit(data)\n",
    "\n",
    "    scores = model.decision_function(data)\n",
    "    preds = model.predict(data)\n",
    "    scaled_scores = 1 - MinMaxScaler().fit_transform(scores.reshape(-1, 1))\n",
    "\n",
    "    # Step 5: Package results\n",
    "    results = df.copy()\n",
    "    results['anomaly_score'] = scores\n",
    "    results['anomaly_likelihood'] = scaled_scores.flatten()\n",
    "    results['is_anomaly'] = (preds == -1).astype(int)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d263f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is mixed-type data with missing values\n",
    "results = detect_anomalies(df_pca, contamination=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46175cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>anomaly_likelihood</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583412</th>\n",
       "      <td>3570412</td>\n",
       "      <td>15605960</td>\n",
       "      <td>430.0</td>\n",
       "      <td>S</td>\n",
       "      <td>11755</td>\n",
       "      <td>174.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>118</td>\n",
       "      <td>-0.381264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452627</th>\n",
       "      <td>3439627</td>\n",
       "      <td>11560420</td>\n",
       "      <td>250.0</td>\n",
       "      <td>S</td>\n",
       "      <td>10024</td>\n",
       "      <td>321.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>144.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>95</td>\n",
       "      <td>-0.369513</td>\n",
       "      <td>0.973614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429406</th>\n",
       "      <td>3416406</td>\n",
       "      <td>10859391</td>\n",
       "      <td>600.0</td>\n",
       "      <td>S</td>\n",
       "      <td>12316</td>\n",
       "      <td>548.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.363836</td>\n",
       "      <td>0.960867</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85794</th>\n",
       "      <td>3072794</td>\n",
       "      <td>1811724</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>15186</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165</td>\n",
       "      <td>-0.362491</td>\n",
       "      <td>0.957845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507373</th>\n",
       "      <td>3494373</td>\n",
       "      <td>13291141</td>\n",
       "      <td>300.0</td>\n",
       "      <td>S</td>\n",
       "      <td>9043</td>\n",
       "      <td>170.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77</td>\n",
       "      <td>-0.351051</td>\n",
       "      <td>0.932156</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "583412        3570412       15605960           430.0         S  11755  174.0   \n",
       "452627        3439627       11560420           250.0         S  10024  321.0   \n",
       "429406        3416406       10859391           600.0         S  12316  548.0   \n",
       "85794         3072794        1811724            50.0         H  15186  512.0   \n",
       "507373        3494373       13291141           300.0         S   9043  170.0   \n",
       "\n",
       "        card3       card4  card5   card6  ...   V335  V336      V337  \\\n",
       "583412  150.0        visa  195.0  credit  ...    0.0   0.0  104060.0   \n",
       "452627  150.0        visa  144.0  credit  ...  250.0   0.0     750.0   \n",
       "429406  150.0        visa  195.0  credit  ...    0.0   0.0   64800.0   \n",
       "85794   150.0  mastercard  224.0   debit  ...    0.0   0.0       0.0   \n",
       "507373  150.0        visa  195.0  credit  ...    0.0   0.0       0.0   \n",
       "\n",
       "            V338      V339 nan_count  anomaly_score  anomaly_likelihood  \\\n",
       "583412  104060.0  104060.0       118      -0.381264            1.000000   \n",
       "452627     750.0     750.0        95      -0.369513            0.973614   \n",
       "429406   64800.0   64800.0        47      -0.363836            0.960867   \n",
       "85794        0.0       0.0       165      -0.362491            0.957845   \n",
       "507373       0.0       0.0        77      -0.351051            0.932156   \n",
       "\n",
       "        is_anomaly  is_fraud  \n",
       "583412           1         0  \n",
       "452627           1         0  \n",
       "429406           1         0  \n",
       "85794            1         0  \n",
       "507373           1         0  \n",
       "\n",
       "[5 rows x 398 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['is_fraud'] = kaggle_set['isFraud']  # Add original fraud labels for comparison\n",
    "results.sort_values(\"anomaly_likelihood\", ascending=False).head() # View top anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1494310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0753, Recall: 0.6460, F1 Score: 0.1349\n"
     ]
    }
   ],
   "source": [
    "results['final_ans']= results['is_fraud']*4 - results['is_anomaly']\n",
    "\n",
    "TP_count = (results['final_ans'] == 3).sum() # True positive is 4-1 = 3\n",
    "FP_count = (results['final_ans'] == -1).sum() # False positive is 0-1 = -1\n",
    "FN_count = (results['final_ans'] == 4).sum() # False negative is 4-0 = 4\n",
    "TN_count = (results['final_ans'] == 0).sum() # True negative is 0-0 = 0\n",
    "\n",
    "Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {Precision:.4f}, Recall: {Recall:.4f}, F1 Score: {F1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to find the optimal contamination level for anomaly detection. \n",
    "# Chosing this level for our entire dataset is a bit of overfitting, \n",
    "# so we ultimately ignore this in favor of using the contamination level based on the fraud proportion in the dataset and biasing towards improved recall.\n",
    "# we selected 30% contamination based on our best judgement to improve recall as much as we reasonably can, but one can adjust this as needed.\n",
    "\n",
    "def find_optimal_contamination(data, contamination_range=np.arange(0.01, 0.08, 0.01)):\n",
    "    best_f1 = 0\n",
    "    best_contamination = 0\n",
    "\n",
    "    for contaminations in contamination_range:\n",
    "        results = detect_anomalies(data, contamination=contaminations)\n",
    "        results['is_fraud'] = kaggle_set['isFraud'] \n",
    "        results['final_ans']= results['is_fraud']*4 - results['is_anomaly']\n",
    "        \n",
    "        TP_count = (results['final_ans'] == 3).sum()\n",
    "        FP_count = (results['final_ans'] == -1).sum()\n",
    "        FN_count = (results['final_ans'] == 4).sum()\n",
    "\n",
    "        Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "        Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "        F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "        print(f\"Contamination: {contaminations:.2f}, F1 Score: {F1_score:.4f}, Precision: {Precision:.4f}, Recall: {Recall:.4f}\")\n",
    "\n",
    "        if F1_score > best_f1:\n",
    "            best_f1 = F1_score\n",
    "            best_contamination = contaminations\n",
    "\n",
    "    return best_contamination, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination: 0.01, F1 Score: 0.1094, Precision: 0.2460, Recall: 0.0703\n",
      "Contamination: 0.02, F1 Score: 0.1663, Precision: 0.2286, Recall: 0.1307\n",
      "Contamination: 0.03, F1 Score: 0.1967, Precision: 0.2130, Recall: 0.1826\n",
      "Contamination: 0.04, F1 Score: 0.2082, Precision: 0.1952, Recall: 0.2232\n",
      "Contamination: 0.05, F1 Score: 0.2130, Precision: 0.1811, Recall: 0.2587\n",
      "Contamination: 0.06, F1 Score: 0.2135, Precision: 0.1690, Recall: 0.2898\n",
      "Contamination: 0.07, F1 Score: 0.2113, Precision: 0.1584, Recall: 0.3170\n",
      "Contamination: 0.08, F1 Score: 0.2086, Precision: 0.1499, Recall: 0.3428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.060000000000000005), np.float64(0.21352681118083286))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_contamination(df_pca)\n",
    "# This will print the F1 scores for different contamination levels and return the best one.\n",
    "# Note: The contamination level is crucial for the performance of the Isolation Forest model.\n",
    "# optimal contamination level is around 0.06 for our dataset (Contamination: 0.06, F1 Score: 0.2135, Precision: 0.1690, Recall: 0.2898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de2473d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.7383\n"
     ]
    }
   ],
   "source": [
    "#we need to calculate the roc-auc\n",
    "from sklearn.metrics import roc_auc_score   \n",
    "#Now calculate the roc-auc score\n",
    "roc_auc = roc_auc_score(results['is_fraud'], results['anomaly_likelihood'])\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
