{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a2cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "txn = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_transaction.csv\")\n",
    "idm = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_identity.csv\")\n",
    "\n",
    "# derive “day” from TransactionDT\n",
    "txn[\"day\"] = (txn[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "\n",
    "# derive card activation date from TransactionDT\n",
    "txn['D1new'] = (txn['TransactionDT'] // (60*60*24)) - txn['D1'] + 2000\n",
    "\n",
    "# drop TransactionDT as it is no longer needed\n",
    "txn.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# merge identity info into transaction table\n",
    "df = txn.merge(idm, on=\"TransactionID\", how=\"left\")\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a435cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3976e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify UIDs\n",
    "def compute_uids(df):\n",
    "\n",
    "    return (\n",
    "        df['card1'].astype(str) + \"_\" +\n",
    "        df['addr1'].astype(str) + \"_\" +\n",
    "        df['D1new'].astype(str) + \"_\" +\n",
    "        df['P_emaildomain'].astype(str) + \"_\" +\n",
    "        df['C1'].astype(str)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39440759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id_12, Unique values: 2\n",
      "One-hot encoding id_12 with 2 unique values\n",
      "Column: id_13, Unique values: 54\n",
      "Column: id_15, Unique values: 3\n",
      "One-hot encoding id_15 with 3 unique values\n",
      "Column: id_16, Unique values: 2\n",
      "One-hot encoding id_16 with 2 unique values\n",
      "Column: id_17, Unique values: 104\n",
      "Column: id_19, Unique values: 522\n",
      "Column: id_20, Unique values: 394\n",
      "Column: id_28, Unique values: 2\n",
      "One-hot encoding id_28 with 2 unique values\n",
      "Column: id_29, Unique values: 2\n",
      "One-hot encoding id_29 with 2 unique values\n",
      "Column: id_31, Unique values: 130\n",
      "Column: id_35, Unique values: 2\n",
      "One-hot encoding id_35 with 2 unique values\n",
      "Column: id_36, Unique values: 2\n",
      "One-hot encoding id_36 with 2 unique values\n",
      "Column: id_37, Unique values: 2\n",
      "One-hot encoding id_37 with 2 unique values\n",
      "Column: id_38, Unique values: 2\n",
      "One-hot encoding id_38 with 2 unique values\n"
     ]
    }
   ],
   "source": [
    "id_cols = [c for c in df.columns if c.startswith(\"id_\")]\n",
    "\n",
    "# but id_01 to id_11 are numerical so need to exclude them\n",
    "id_cat_cols = [c for c in id_cols if not c.startswith(\"id_0\")]\n",
    "id_cat_cols.remove(\"id_11\")  # id_11 is a numerical column\n",
    "\n",
    "# extract high cardinality categorical ID columns\n",
    "id_high_card_cols = [c for c in id_cat_cols if df[c].nunique() > 10]\n",
    "\n",
    "# one-hot encode categorical features with low cardinality\n",
    "for c in id_cat_cols:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6c9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical ID columns: ['id_13', 'id_17', 'id_19', 'id_20', 'id_31']\n"
     ]
    }
   ],
   "source": [
    "print(f\"High cardinality categorical ID columns: {id_high_card_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2549cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# include id_01 to id_11 as they are numerical\n",
    "id_num_cols = [c for c in id_cols if c not in id_cat_cols]\n",
    "num_cols.extend(id_num_cols)\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card1, card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ProductCD, Unique values: 5\n",
      "One-hot encoding ProductCD with 5 unique values\n",
      "Column: card4, Unique values: 4\n",
      "One-hot encoding card4 with 4 unique values\n",
      "Column: card6, Unique values: 4\n",
      "One-hot encoding card6 with 4 unique values\n",
      "Column: P_emaildomain, Unique values: 59\n",
      "Column: R_emaildomain, Unique values: 60\n",
      "Column: M1, Unique values: 2\n",
      "One-hot encoding M1 with 2 unique values\n",
      "Column: M2, Unique values: 2\n",
      "One-hot encoding M2 with 2 unique values\n",
      "Column: M3, Unique values: 2\n",
      "One-hot encoding M3 with 2 unique values\n",
      "Column: M4, Unique values: 3\n",
      "One-hot encoding M4 with 3 unique values\n",
      "Column: M5, Unique values: 2\n",
      "One-hot encoding M5 with 2 unique values\n",
      "Column: M6, Unique values: 2\n",
      "One-hot encoding M6 with 2 unique values\n",
      "Column: M7, Unique values: 2\n",
      "One-hot encoding M7 with 2 unique values\n",
      "Column: M8, Unique values: 2\n",
      "One-hot encoding M8 with 2 unique values\n",
      "Column: M9, Unique values: 2\n",
      "One-hot encoding M9 with 2 unique values\n",
      "Column: id_31, Unique values: 130\n",
      "Column: DeviceType, Unique values: 2\n",
      "One-hot encoding DeviceType with 2 unique values\n",
      "Column: DeviceInfo, Unique values: 1786\n",
      "Column: card1, Unique values: 13553\n",
      "Column: card2, Unique values: 500\n",
      "Column: card3, Unique values: 114\n",
      "Column: card5, Unique values: 119\n",
      "Column: addr1, Unique values: 332\n",
      "Column: addr2, Unique values: 74\n"
     ]
    }
   ],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# extract high cardinality categorical columns\n",
    "high_card_cols = [c for c in cat_cols_rem if df[c].nunique() > 10]\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a22e182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_13', 'id_17', 'id_19', 'id_20', 'id_31']\n"
     ]
    }
   ],
   "source": [
    "print(id_high_card_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b12be534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical columns: ['P_emaildomain', 'id_20', 'card3', 'DeviceInfo', 'card2', 'card5', 'addr2', 'card1', 'id_17', 'id_19', 'id_13', 'R_emaildomain', 'addr1', 'id_31']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# include the high cardinality categorical ID columns but remove duplicates\n",
    "high_card_cols = list(set(high_card_cols + id_high_card_cols))  # remove duplicates\n",
    "print(f\"High cardinality categorical columns: {high_card_cols}\")\n",
    "print(len(high_card_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>M2_T</th>\n",
       "      <th>M3_T</th>\n",
       "      <th>M4_M1</th>\n",
       "      <th>M4_M2</th>\n",
       "      <th>M5_T</th>\n",
       "      <th>M6_T</th>\n",
       "      <th>M7_T</th>\n",
       "      <th>M8_T</th>\n",
       "      <th>M9_T</th>\n",
       "      <th>DeviceType_mobile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  card1  card2  card3  card5  addr1  addr2  dist1  \\\n",
       "0        0            68.5  13926    NaN  150.0  142.0  315.0   87.0   19.0   \n",
       "1        0            29.0   2755  404.0  150.0  102.0  325.0   87.0    8.0   \n",
       "2        0            59.0   4663  490.0  150.0  166.0  330.0   87.0  287.0   \n",
       "3        0            50.0  18132  567.0  150.0  117.0  476.0   87.0    8.0   \n",
       "4        0            50.0   4497  514.0  150.0  102.0  420.0   87.0    8.0   \n",
       "\n",
       "  P_emaildomain  ... M2_T  M3_T  M4_M1  M4_M2  M5_T  M6_T  M7_T  M8_T  M9_T  \\\n",
       "0           NaN  ...    1     1      0      1     0     1     0     0     0   \n",
       "1     gmail.com  ...    0     0      0      0     1     1     0     0     0   \n",
       "2   outlook.com  ...    1     1      0      0     0     0     0     0     0   \n",
       "3     yahoo.com  ...    0     0      0      0     1     0     0     0     0   \n",
       "4     gmail.com  ...    0     0      0      0     0     0     0     0     0   \n",
       "\n",
       "   DeviceType_mobile  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  1  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b2c1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode high cardinality categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Store encoders for later inference\n",
    "encoders = {}\n",
    "for col in high_card_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b823102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all columns from high cardinality categorical columns\n",
    "other_cols = [c for c in df.columns if c not in high_card_cols]\n",
    "other_cols.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8af0b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 250514 normal UIDs → 454451 transactions\n",
      "CV set on 74791 UIDs → 136089 transactions\n"
     ]
    }
   ],
   "source": [
    "# Create training and CV sets by UID\n",
    "\n",
    "# Compute UIDs for every transaction\n",
    "df['UID'] = compute_uids(df)   # assumes compute_uids(df) returns a Series of length df\n",
    "\n",
    "# Label each UID as fraud if any transaction is fraudulent\n",
    "uid_labels = df.groupby('UID')['isFraud'].max().rename('UID_isFraud')\n",
    "df = df.merge(uid_labels, left_on='UID', right_index=True)\n",
    "\n",
    "# Split UIDs into “normal” vs. “fraud”\n",
    "normal_uids = uid_labels[uid_labels == 0].index\n",
    "fraud_uids  = uid_labels[uid_labels == 1].index\n",
    "\n",
    "# Hold out 20% of normal UIDs for CV:\n",
    "rng = np.random.RandomState(42)\n",
    "hold_normals = rng.choice(normal_uids, size=int(0.2 * len(normal_uids)), replace=False)\n",
    "\n",
    "# TRAIN on the remaining 80% normal UIDs\n",
    "train_uids = np.setdiff1d(normal_uids, hold_normals)\n",
    "train_df   = df[df['UID'].isin(train_uids)].copy()\n",
    "\n",
    "# CV on all fraud UIDs + held‐out normals\n",
    "cv_uids = np.concatenate([fraud_uids, hold_normals])\n",
    "cv_df   = df[df['UID'].isin(cv_uids)].copy()\n",
    "\n",
    "print(f\"Training on {len(train_uids)} normal UIDs → {train_df.shape[0]} transactions\")\n",
    "print(f\"CV set on {len(cv_uids)} UIDs → {cv_df.shape[0]} transactions\")\n",
    "\n",
    "# Build feature‐matrices (drop isFraud & UID)\n",
    "X_train = train_df.drop(['isFraud','UID'], axis=1)\n",
    "X_cv    = cv_df   .drop(['isFraud','UID'], axis=1)\n",
    "y_cv    = cv_df['isFraud'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9b9801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal UIDs available: 313142\n",
      "Number of fraud UIDs available:  12163\n",
      "UIDs in training set:            250514\n",
      "UIDs in CV set:                  74791\n"
     ]
    }
   ],
   "source": [
    "# how many UIDs in each set?\n",
    "n_normal_uids = len(normal_uids)\n",
    "n_fraud_uids  = len(fraud_uids)\n",
    "n_train_uids  = train_df['UID'].nunique()\n",
    "n_cv_uids     = cv_df  ['UID'].nunique()\n",
    "\n",
    "print(f\"Number of normal UIDs available: {n_normal_uids}\")\n",
    "print(f\"Number of fraud UIDs available:  {n_fraud_uids}\")\n",
    "print(f\"UIDs in training set:            {n_train_uids}\")\n",
    "print(f\"UIDs in CV set:                  {n_cv_uids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "non_one_hot_cols = [col for col in X_train.columns if col not in one_hot_cols and col not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_cv_scaled    = X_cv.copy()\n",
    "\n",
    "X_train_scaled[non_one_hot_cols] = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_cv_scaled[non_one_hot_cols]    = scaler.transform(X_cv[non_one_hot_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e6f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_num_train = torch.tensor(X_train_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_train = torch.tensor(X_train_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_val = torch.tensor(X_cv_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_val = torch.tensor(X_cv_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "y_valid = torch.FloatTensor(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "911478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_num_train, X_cat_train, X_num_train, X_cat_train)\n",
    "val_ds   = TensorDataset(X_num_val, X_cat_val, X_num_val, X_cat_val)\n",
    "\n",
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e3af693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UID‐aware DataLoader for CV aggregation\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extract the unique CV UIDs and build a mapping → index\n",
    "unique_cv_uids = cv_df['UID'].unique()\n",
    "uid2idx        = {uid: i for i, uid in enumerate(unique_cv_uids)}\n",
    "\n",
    "# Map your UID column to these integer indices\n",
    "uid_idx_arr = cv_df['UID'].map(uid2idx).astype(int).values\n",
    "\n",
    "# Now you can build a LongTensor\n",
    "uid_cv_tensor = torch.tensor(uid_idx_arr, dtype=torch.long)\n",
    "\n",
    "# Create your dataset & loader as before\n",
    "uid_eval_ds     = TensorDataset(X_num_val, X_cat_val, uid_cv_tensor)\n",
    "uid_eval_loader = DataLoader(uid_eval_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73448abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2):\n",
    "        super(AutoEncoderWithEmbeddings, self).__init__()\n",
    "        \n",
    "        # Create embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, min(50, (num_categories + 1)//2))\n",
    "            for num_categories in cat_cardinalities\n",
    "        ])\n",
    "        \n",
    "        emb_size_total = sum([emb.embedding_dim for emb in self.embeddings])\n",
    "        total_input_size = num_numeric + emb_size_total\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(total_input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, hidden2)\n",
    "        self.fc5 = nn.Linear(hidden2, hidden1)\n",
    "        self.fc6 = nn.Linear(hidden1, total_input_size)\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Embed categorical variables\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embeds = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        # Concatenate numerical + embeddings\n",
    "        x = torch.cat([x_num, embeds], dim=1)\n",
    "\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a15fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fe09b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9981a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numeric features (including one-hot encodings)\n",
    "num_numeric = len(other_cols)\n",
    "\n",
    "# Cardinalities (unique values) of each categorical feature\n",
    "cat_cardinalities = [df[col].nunique() for col in high_card_cols]\n",
    "\n",
    "# Initialize the embedding autoencoder model\n",
    "seed_everything(SEED)\n",
    "model = AutoEncoderWithEmbeddings(num_numeric=num_numeric, \n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            hidden1=128, hidden2=64, code_size=8, \n",
    "                            dropout_rate=0.2)\n",
    "\n",
    "losses = per_sample_mse(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float32(0.71192455), np.float32(0.7349194), np.float32(0.68293494), np.float32(0.7178748), np.float32(0.76193386)]\n",
      "1.339104\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66da7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,generator,criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        batch_losses.append(loss.item())\n",
    "    mean_loss = np.mean(batch_losses)    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04b30bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "    \n",
    "    def continue_training(self,current_score):\n",
    "        if self.best_score > current_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(\"New best score:\", current_score)\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(self.counter, \" iterations since best score.\")\n",
    "                \n",
    "        return self.counter <= self.patience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4465904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    #Setting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    \n",
    "    #Training loop\n",
    "    start_time=time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, y_true_full)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        #showing last training loss after each epoch\n",
    "        all_train_losses.append(np.mean(train_loss))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        #evaluating the model on the test set after each epoch    \n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.continue_training(valid_loss):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "    training_execution_time=time.time()-start_time\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d616349",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef8007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.6050630776664695\n",
      "valid loss: 0.8247278764059669\n",
      "New best score: 0.8247278764059669\n",
      "\n",
      "Epoch 1: train loss: 0.46598865763024166\n",
      "valid loss: 0.7771255816508057\n",
      "New best score: 0.7771255816508057\n",
      "\n",
      "Epoch 2: train loss: 0.42154230090143446\n",
      "valid loss: 0.7228615339985467\n",
      "New best score: 0.7228615339985467\n",
      "\n",
      "Epoch 3: train loss: 0.3828485046286841\n",
      "valid loss: 0.6553017530114131\n",
      "New best score: 0.6553017530114131\n",
      "\n",
      "Epoch 4: train loss: 0.34616664870902225\n",
      "valid loss: 0.5985279115183013\n",
      "New best score: 0.5985279115183013\n",
      "\n",
      "Epoch 5: train loss: 0.31369785125459637\n",
      "valid loss: 0.5678653494083792\n",
      "New best score: 0.5678653494083792\n",
      "\n",
      "Epoch 6: train loss: 0.29016324369287166\n",
      "valid loss: 0.5427642594042578\n",
      "New best score: 0.5427642594042578\n",
      "\n",
      "Epoch 7: train loss: 0.27126387380869\n",
      "valid loss: 0.5227748528683096\n",
      "New best score: 0.5227748528683096\n",
      "\n",
      "Epoch 8: train loss: 0.2555021621656042\n",
      "valid loss: 0.5040838949774441\n",
      "New best score: 0.5040838949774441\n",
      "\n",
      "Epoch 9: train loss: 0.241418777587446\n",
      "valid loss: 0.48792864376665057\n",
      "New best score: 0.48792864376665057\n",
      "\n",
      "Epoch 10: train loss: 0.2291585565754422\n",
      "valid loss: 0.4731139711531481\n",
      "New best score: 0.4731139711531481\n",
      "\n",
      "Epoch 11: train loss: 0.21773150063178562\n",
      "valid loss: 0.4594627625231904\n",
      "New best score: 0.4594627625231904\n",
      "\n",
      "Epoch 12: train loss: 0.20739139049305572\n",
      "valid loss: 0.43989230462707074\n",
      "New best score: 0.43989230462707074\n",
      "\n",
      "Epoch 13: train loss: 0.19799683446081373\n",
      "valid loss: 0.4212866256335624\n",
      "New best score: 0.4212866256335624\n",
      "\n",
      "Epoch 14: train loss: 0.1889016175021728\n",
      "valid loss: 0.40105497198445456\n",
      "New best score: 0.40105497198445456\n",
      "\n",
      "Epoch 15: train loss: 0.1822325367782567\n",
      "valid loss: 0.3893978569404523\n",
      "New best score: 0.3893978569404523\n",
      "\n",
      "Epoch 16: train loss: 0.17615349401996747\n",
      "valid loss: 0.3738739859956996\n",
      "New best score: 0.3738739859956996\n",
      "\n",
      "Epoch 17: train loss: 0.17095428073426355\n",
      "valid loss: 0.3619252860097957\n",
      "New best score: 0.3619252860097957\n",
      "\n",
      "Epoch 18: train loss: 0.16579152984326487\n",
      "valid loss: 0.3535056145007449\n",
      "New best score: 0.3535056145007449\n",
      "\n",
      "Epoch 19: train loss: 0.16176470011674068\n",
      "valid loss: 0.34100852954320443\n",
      "New best score: 0.34100852954320443\n",
      "\n",
      "Epoch 20: train loss: 0.1572931812119645\n",
      "valid loss: 0.33489860008869854\n",
      "New best score: 0.33489860008869854\n",
      "\n",
      "Epoch 21: train loss: 0.1550084275638198\n",
      "valid loss: 0.32648146090874997\n",
      "New best score: 0.32648146090874997\n",
      "\n",
      "Epoch 22: train loss: 0.15121400602128324\n",
      "valid loss: 0.32020128944090437\n",
      "New best score: 0.32020128944090437\n",
      "\n",
      "Epoch 23: train loss: 0.1488546618000344\n",
      "valid loss: 0.3156555542036107\n",
      "New best score: 0.3156555542036107\n",
      "\n",
      "Epoch 24: train loss: 0.14615360954524698\n",
      "valid loss: 0.3099824896544442\n",
      "New best score: 0.3099824896544442\n",
      "\n",
      "Epoch 25: train loss: 0.14304329950765177\n",
      "valid loss: 0.3056733009118335\n",
      "New best score: 0.3056733009118335\n",
      "\n",
      "Epoch 26: train loss: 0.14152552271345714\n",
      "valid loss: 0.3008333451597762\n",
      "New best score: 0.3008333451597762\n",
      "\n",
      "Epoch 27: train loss: 0.1415842005580261\n",
      "valid loss: 0.29962454475742534\n",
      "New best score: 0.29962454475742534\n",
      "\n",
      "Epoch 28: train loss: 0.13823677627957082\n",
      "valid loss: 0.29806668246935186\n",
      "New best score: 0.29806668246935186\n",
      "\n",
      "Epoch 29: train loss: 0.13886558976829858\n",
      "valid loss: 0.29160357980911894\n",
      "New best score: 0.29160357980911894\n",
      "\n",
      "Epoch 30: train loss: 0.13613536199638704\n",
      "valid loss: 0.29101271451191796\n",
      "New best score: 0.29101271451191796\n",
      "\n",
      "Epoch 31: train loss: 0.1335236454667809\n",
      "valid loss: 0.28902245786293107\n",
      "New best score: 0.28902245786293107\n",
      "\n",
      "Epoch 32: train loss: 0.13329037067455216\n",
      "valid loss: 0.2860872036308274\n",
      "New best score: 0.2860872036308274\n",
      "\n",
      "Epoch 33: train loss: 0.13201534626418138\n",
      "valid loss: 0.2864054533323847\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 34: train loss: 0.12918994298020192\n",
      "valid loss: 0.282662300799126\n",
      "New best score: 0.282662300799126\n",
      "\n",
      "Epoch 35: train loss: 0.12965531530580274\n",
      "valid loss: 0.2830536006144563\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 36: train loss: 0.12757730385003327\n",
      "valid loss: 0.28387360011500523\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 37: train loss: 0.12687028550148546\n",
      "valid loss: 0.2789945583090298\n",
      "New best score: 0.2789945583090298\n",
      "\n",
      "Epoch 38: train loss: 0.12542070519716084\n",
      "valid loss: 0.27878713753438533\n",
      "New best score: 0.27878713753438533\n",
      "\n",
      "Epoch 39: train loss: 0.12723821740563926\n",
      "valid loss: 0.2772385637394916\n",
      "New best score: 0.2772385637394916\n",
      "\n",
      "Epoch 40: train loss: 0.12433046086582246\n",
      "valid loss: 0.27643628970236706\n",
      "New best score: 0.27643628970236706\n",
      "\n",
      "Epoch 41: train loss: 0.12524711870093336\n",
      "valid loss: 0.2752335994483385\n",
      "New best score: 0.2752335994483385\n",
      "\n",
      "Epoch 42: train loss: 0.12342728452908026\n",
      "valid loss: 0.27384909399245916\n",
      "New best score: 0.27384909399245916\n",
      "\n",
      "Epoch 43: train loss: 0.12325479491269803\n",
      "valid loss: 0.2712183566413876\n",
      "New best score: 0.2712183566413876\n",
      "\n",
      "Epoch 44: train loss: 0.12192879918425738\n",
      "valid loss: 0.2727307116794855\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 45: train loss: 0.12174902199389967\n",
      "valid loss: 0.2707940254892622\n",
      "New best score: 0.2707940254892622\n",
      "\n",
      "Epoch 46: train loss: 0.12070412422018545\n",
      "valid loss: 0.26821677585629594\n",
      "New best score: 0.26821677585629594\n",
      "\n",
      "Epoch 47: train loss: 0.12070190555810391\n",
      "valid loss: 0.2675651039900188\n",
      "New best score: 0.2675651039900188\n",
      "\n",
      "Epoch 48: train loss: 0.1208142884713304\n",
      "valid loss: 0.26768259169127706\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 49: train loss: 0.11974637754060127\n",
      "valid loss: 0.26591459263984424\n",
      "New best score: 0.26591459263984424\n",
      "\n",
      "Epoch 50: train loss: 0.1188852078541442\n",
      "valid loss: 0.265578646044758\n",
      "New best score: 0.265578646044758\n",
      "\n",
      "Epoch 51: train loss: 0.1188100160242201\n",
      "valid loss: 0.2626133015505353\n",
      "New best score: 0.2626133015505353\n",
      "\n",
      "Epoch 52: train loss: 0.11872167425515416\n",
      "valid loss: 0.26413521988499433\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 53: train loss: 0.11692466651433492\n",
      "valid loss: 0.26390284686384347\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 54: train loss: 0.11692411857782989\n",
      "valid loss: 0.26168522017454743\n",
      "New best score: 0.26168522017454743\n",
      "\n",
      "Epoch 55: train loss: 0.11811163242033741\n",
      "valid loss: 0.2615804464409226\n",
      "New best score: 0.2615804464409226\n",
      "\n",
      "Epoch 56: train loss: 0.1172954202923286\n",
      "valid loss: 0.261614568083358\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 57: train loss: 0.11542879413343496\n",
      "valid loss: 0.25904702808343827\n",
      "New best score: 0.25904702808343827\n",
      "\n",
      "Epoch 58: train loss: 0.115007791735299\n",
      "valid loss: 0.2634433301347763\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 59: train loss: 0.11482813054913873\n",
      "valid loss: 0.2573669081959957\n",
      "New best score: 0.2573669081959957\n",
      "\n",
      "Epoch 60: train loss: 0.11424604782348012\n",
      "valid loss: 0.25739269688221295\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 61: train loss: 0.11482048689110859\n",
      "valid loss: 0.2578408858997929\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 62: train loss: 0.11349438959816555\n",
      "valid loss: 0.25583212552382084\n",
      "New best score: 0.25583212552382084\n",
      "\n",
      "Epoch 63: train loss: 0.11371543000846564\n",
      "valid loss: 0.25601670878442156\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 64: train loss: 0.11428020970040076\n",
      "valid loss: 0.25390173910759894\n",
      "New best score: 0.25390173910759894\n",
      "\n",
      "Epoch 65: train loss: 0.1136215003207326\n",
      "valid loss: 0.2547007690050772\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 66: train loss: 0.11291794676904206\n",
      "valid loss: 0.25376617516341965\n",
      "New best score: 0.25376617516341965\n",
      "\n",
      "Epoch 67: train loss: 0.11207207847755772\n",
      "valid loss: 0.25285291894430056\n",
      "New best score: 0.25285291894430056\n",
      "\n",
      "Epoch 68: train loss: 0.11247053532651416\n",
      "valid loss: 0.2502277768252039\n",
      "New best score: 0.2502277768252039\n",
      "\n",
      "Epoch 69: train loss: 0.11191056062140174\n",
      "valid loss: 0.2503891109972072\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 70: train loss: 0.11202263088712285\n",
      "valid loss: 0.2511022189085869\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 71: train loss: 0.11119508622465907\n",
      "valid loss: 0.25203045796518936\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 72: train loss: 0.11105443667284809\n",
      "valid loss: 0.25113218550787386\n",
      "4  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,train_loader,val_loader,optimizer,criterion,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9684c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UID-level reconstruction errors\n",
    "def compute_uid_errors(model, loader, idx2uid):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    uid_errors = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, uid_idxs in loader:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "            # rebuild full “true” vector\n",
    "            true_embeds = [model.embeddings[i](xb_cat[:, i])\n",
    "                           for i in range(len(model.embeddings))]\n",
    "            y_true_full = torch.cat([xb_num] + true_embeds, dim=1)\n",
    "\n",
    "            losses = criterion(y_pred, y_true_full).mean(dim=1).cpu().numpy()\n",
    "            for idx, err in zip(uid_idxs.cpu().numpy(), losses):\n",
    "                actual_uid = idx2uid[idx]\n",
    "                uid_errors[actual_uid].append(err)\n",
    "\n",
    "    return uid_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b21a2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC: 0.7976\n"
     ]
    }
   ],
   "source": [
    "# evaluate UID‐level reconstruction errors\n",
    "\n",
    "idx2uid    = list(unique_cv_uids)\n",
    "uid_errors = compute_uid_errors(model, uid_eval_loader, idx2uid)\n",
    "\n",
    "# average per‐UID\n",
    "uid_avg = {u: np.mean(errs) for u, errs in uid_errors.items()}\n",
    "uid_df  = pd.DataFrame.from_dict(uid_avg, orient='index', \n",
    "                                  columns=['avg_error'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "auc = roc_auc_score(uid_df['true_label'], uid_df['avg_error'])\n",
    "print(f\"UID‐level ROC AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d66bc6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 70th percentile train-error threshold = 0.7234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "train_errs = per_sample_mse(model, train_loader)\n",
    "thresh     = np.percentile(train_errs, 70)  # 70th percentile as threshold\n",
    "uid_df['pred'] = (uid_df['avg_error'] >= thresh).astype(int)\n",
    "recall   = recall_score(uid_df['true_label'], uid_df['pred'])    # binary‐class recall\n",
    "print(f\"Recall at 70th percentile train-error threshold = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc880a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC (fraction rule):       0.7331\n",
      "UID‐level recall (majority‐vote rule): 0.7258\n"
     ]
    }
   ],
   "source": [
    "# UID‐level evaluation via fraction‐rule\n",
    "\n",
    "# Recompute uid_errors if needed\n",
    "idx2uid    = list(unique_cv_uids)\n",
    "uid_errors = compute_uid_errors(model, uid_eval_loader, idx2uid)\n",
    "\n",
    "# Build train‐error threshold (70th pct of per‐sample errs)\n",
    "train_errs = per_sample_mse(model, train_loader)\n",
    "thresh     = np.percentile(train_errs, 70)  # 70th percentile as threshold\n",
    "\n",
    "# For each UID, compute fraction of its txns flagged as fraud\n",
    "uid_frac = {\n",
    "    uid: np.mean(np.array(errs) >= thresh)\n",
    "    for uid, errs in uid_errors.items()\n",
    "}\n",
    "\n",
    "# Build DataFrame and true labels\n",
    "uid_df = pd.DataFrame.from_dict(uid_frac, orient='index',\n",
    "                                columns=['fraud_frac'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# Classify UID as fraud if majority of its txns exceed threshold\n",
    "uid_df['pred'] = (uid_df['fraud_frac'] >= 0.50).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc_frac = roc_auc_score(uid_df['true_label'], uid_df['fraud_frac'])\n",
    "recall_frac = recall_score(uid_df['true_label'], uid_df['pred'])\n",
    "\n",
    "print(f\"UID‐level ROC AUC (fraction rule):       {auc_frac:.4f}\")\n",
    "print(f\"UID‐level recall (majority‐vote rule): {recall_frac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
