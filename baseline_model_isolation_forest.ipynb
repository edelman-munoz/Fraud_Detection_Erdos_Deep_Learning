{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfd1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a89b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-D90o63nu_b-N1h-pNUlIRkUMVuP5JLs\n",
      "From (redirected): https://drive.google.com/uc?id=1-D90o63nu_b-N1h-pNUlIRkUMVuP5JLs&confirm=t&uuid=82dc73a6-cfd8-4bc5-954c-070ab9184192\n",
      "To: /Users/sara/Desktop/data/ieee-fraud/train_transaction.csv\n",
      "100%|██████████| 683M/683M [19:05<00:00, 597kB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'idm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     28\u001b[39m     kaggle_set = pd.read_csv(txn_path, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m, engine=\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain_identity  :\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43midm\u001b[49m.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'idm' is not defined"
     ]
    }
   ],
   "source": [
    "# Load IEEE-CIS CSVs from Google Drive by FILE ID\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure gdown is available\n",
    "try:\n",
    "    import gdown\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gdown\"])\n",
    "    import gdown\n",
    "\n",
    "# File IDs to pull from Google Drive\n",
    "TXN_ID = \"1-D90o63nu_b-N1h-pNUlIRkUMVuP5JLs\"\n",
    "IDM_ID = \"1tUZyy06wbS9l-3yTaXC_O8TgbQLjrg6G\"\n",
    "\n",
    "DATA_DIR = Path(\"./data/ieee-fraud\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download if missing\n",
    "txn_path = DATA_DIR / \"train_transaction.csv\"\n",
    "\n",
    "if not txn_path.exists(): gdown.download(id=TXN_ID, output=str(txn_path), quiet=False)\n",
    "\n",
    "# Read CSVs (with a forgiving parser fallback)\n",
    "try:\n",
    "    kaggle_set = pd.read_csv(txn_path, low_memory=False)\n",
    "except Exception:\n",
    "    kaggle_set = pd.read_csv(txn_path, low_memory=False, engine=\"python\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4bd638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_identity  : (590540, 394)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_identity  :\", kaggle_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from my computer. Cell is Off rn becuase we're loading from drive, in this case data are tabluar and of mixed types\n",
    "#kaggle_set = pd.read_csv('/Users/sara/Desktop/train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0811b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the proportion of fraud cases so we can think about what would be a good final contamination level\n",
    "fraud_proportion = kaggle_set['isFraud'].sum()/kaggle_set.shape[0]  \n",
    "\n",
    "# Drop the labels so they can't be used as a factor\n",
    "df = kaggle_set.copy().drop(columns=['isFraud']) \n",
    "\n",
    "# Add a count of nans in columns. Not using one-hot encoding because it will cluster fraud cases together. \n",
    "df['nan_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "# Separate into categorical and numerical factors\n",
    "categorical_df = df.select_dtypes(include=['object']).copy()\n",
    "#fill missing values in categorical columns with the mode of the column\n",
    "for column in categorical_df.columns:\n",
    "    if categorical_df[column].isnull().any(): \n",
    "        categorical_df[column] = categorical_df[column].fillna(categorical_df[column].mode()[0])\n",
    "        \n",
    "\n",
    "categorical_df = categorical_df.apply(lambda x: LabelEncoder().fit_transform(x.astype(str)))\n",
    "\n",
    "numerical_df = df.select_dtypes(exclude=['object']).copy()\n",
    "# Fill missing values in numerical columns with the mean of the column\n",
    "for column in numerical_df.columns:\n",
    "    # Convert numerical columns to float type and Z-normalize\n",
    "    numerical_df[column] = numerical_df[column].astype(float)\n",
    "    numerical_df[column] = (numerical_df[column]- numerical_df[column].mean())/ numerical_df[column].std()\n",
    "\n",
    "    # Fill missing values with the mean of the column (0 after normalization)\n",
    "    numerical_df[column] = numerical_df[column].fillna(numerical_df[column].mean())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea71e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PCA to reduce dimensionality\n",
    "pca = PCA(n_components=0.95)  # Keep 79% of variance (I think this was Adrian's number but correct me if I'm wrong)\n",
    "our_pca = pca.fit_transform(numerical_df.drop(columns=['nan_count']) )\n",
    "\n",
    "df_pca = pd.DataFrame(our_pca, columns=[f'PC{i+1}' for i in range(our_pca.shape[1])])\n",
    "# add the categorical and nancount columns back to the PCA dataframe\n",
    "df_pca = pd.concat([df_pca, categorical_df.reset_index(drop=True)], axis = 1)\n",
    "df_pca['nan_count'] = numerical_df['nan_count'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8884be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(data, contamination, random_state=42):\n",
    "#takes a DataFrame, applies Isolation Forest, and returns a DataFrame with anomaly scores and predictions.\n",
    "\n",
    "    # Step 4: Isolation Forest\n",
    "    model = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    model.fit(data)\n",
    "\n",
    "    scores = model.decision_function(data)\n",
    "    preds = model.predict(data)\n",
    "    scaled_scores = 1 - MinMaxScaler().fit_transform(scores.reshape(-1, 1))\n",
    "\n",
    "    # Step 5: Package results\n",
    "    results = df.copy()\n",
    "    results['anomaly_score'] = scores\n",
    "    results['anomaly_likelihood'] = scaled_scores.flatten()\n",
    "    results['is_anomaly'] = (preds == -1).astype(int)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d263f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's where we actually run the model. We set the contamination to .3 for reasons discussed later.\n",
    "# df is mixed-type data with missing values\n",
    "results = detect_anomalies(df_pca, contamination=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46175cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>anomaly_score</th>\n",
       "      <th>anomaly_likelihood</th>\n",
       "      <th>is_anomaly</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>583412</th>\n",
       "      <td>3570412</td>\n",
       "      <td>15605960</td>\n",
       "      <td>430.0</td>\n",
       "      <td>S</td>\n",
       "      <td>11755</td>\n",
       "      <td>174.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>104060.0</td>\n",
       "      <td>118</td>\n",
       "      <td>-0.381264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452627</th>\n",
       "      <td>3439627</td>\n",
       "      <td>11560420</td>\n",
       "      <td>250.0</td>\n",
       "      <td>S</td>\n",
       "      <td>10024</td>\n",
       "      <td>321.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>144.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>95</td>\n",
       "      <td>-0.369513</td>\n",
       "      <td>0.973614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429406</th>\n",
       "      <td>3416406</td>\n",
       "      <td>10859391</td>\n",
       "      <td>600.0</td>\n",
       "      <td>S</td>\n",
       "      <td>12316</td>\n",
       "      <td>548.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.363836</td>\n",
       "      <td>0.960867</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85794</th>\n",
       "      <td>3072794</td>\n",
       "      <td>1811724</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>15186</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165</td>\n",
       "      <td>-0.362491</td>\n",
       "      <td>0.957845</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507373</th>\n",
       "      <td>3494373</td>\n",
       "      <td>13291141</td>\n",
       "      <td>300.0</td>\n",
       "      <td>S</td>\n",
       "      <td>9043</td>\n",
       "      <td>170.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>195.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77</td>\n",
       "      <td>-0.351051</td>\n",
       "      <td>0.932156</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "583412        3570412       15605960           430.0         S  11755  174.0   \n",
       "452627        3439627       11560420           250.0         S  10024  321.0   \n",
       "429406        3416406       10859391           600.0         S  12316  548.0   \n",
       "85794         3072794        1811724            50.0         H  15186  512.0   \n",
       "507373        3494373       13291141           300.0         S   9043  170.0   \n",
       "\n",
       "        card3       card4  card5   card6  ...   V335  V336      V337  \\\n",
       "583412  150.0        visa  195.0  credit  ...    0.0   0.0  104060.0   \n",
       "452627  150.0        visa  144.0  credit  ...  250.0   0.0     750.0   \n",
       "429406  150.0        visa  195.0  credit  ...    0.0   0.0   64800.0   \n",
       "85794   150.0  mastercard  224.0   debit  ...    0.0   0.0       0.0   \n",
       "507373  150.0        visa  195.0  credit  ...    0.0   0.0       0.0   \n",
       "\n",
       "            V338      V339 nan_count  anomaly_score  anomaly_likelihood  \\\n",
       "583412  104060.0  104060.0       118      -0.381264            1.000000   \n",
       "452627     750.0     750.0        95      -0.369513            0.973614   \n",
       "429406   64800.0   64800.0        47      -0.363836            0.960867   \n",
       "85794        0.0       0.0       165      -0.362491            0.957845   \n",
       "507373       0.0       0.0        77      -0.351051            0.932156   \n",
       "\n",
       "        is_anomaly  is_fraud  \n",
       "583412           1         0  \n",
       "452627           1         0  \n",
       "429406           1         0  \n",
       "85794            1         0  \n",
       "507373           1         0  \n",
       "\n",
       "[5 rows x 398 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['is_fraud'] = kaggle_set['isFraud']  # Add original fraud labels for comparison\n",
    "results.sort_values(\"anomaly_likelihood\", ascending=False).head() # View top anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1494310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0753, Recall: 0.6460, F1 Score: 0.1349\n"
     ]
    }
   ],
   "source": [
    "results['final_ans']= results['is_fraud']*4 - results['is_anomaly']\n",
    "\n",
    "TP_count = (results['final_ans'] == 3).sum() # True positive is 4-1 = 3\n",
    "FP_count = (results['final_ans'] == -1).sum() # False positive is 0-1 = -1\n",
    "FN_count = (results['final_ans'] == 4).sum() # False negative is 4-0 = 4\n",
    "TN_count = (results['final_ans'] == 0).sum() # True negative is 0-0 = 0\n",
    "\n",
    "Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {Precision:.4f}, Recall: {Recall:.4f}, F1 Score: {F1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we do an analysis of the optimal contamination level for anomaly detection. \n",
    "# Chosing this level based on our entire dataset is a bit of overfitting, \n",
    "# so we ultimately ignore this in favor of using the contamination level based on the fraud proportion in the dataset and biasing towards improved recall.\n",
    "# We selected 30% contamination based on our best judgement to improve recall as much as we reasonably can, but one can adjust this as needed.\n",
    "\n",
    "def find_optimal_contamination(data, contamination_range=np.arange(0.01, 0.5, 0.01)):\n",
    "    best_f1 = 0\n",
    "    best_contamination = 0\n",
    "\n",
    "    for contaminations in contamination_range:\n",
    "        results = detect_anomalies(data, contamination=contaminations)\n",
    "        results['is_fraud'] = kaggle_set['isFraud'] \n",
    "        results['final_ans']= results['is_fraud']*4 - results['is_anomaly']\n",
    "        \n",
    "        TP_count = (results['final_ans'] == 3).sum()\n",
    "        FP_count = (results['final_ans'] == -1).sum()\n",
    "        FN_count = (results['final_ans'] == 4).sum()\n",
    "\n",
    "        Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "        Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "        F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "        print(f\"Contamination: {contaminations:.2f}, F1 Score: {F1_score:.4f}, Precision: {Precision:.4f}, Recall: {Recall:.4f}\")\n",
    "\n",
    "        if F1_score > best_f1:\n",
    "            best_f1 = F1_score\n",
    "            best_contamination = contaminations\n",
    "\n",
    "    return best_contamination, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination: 0.01, F1 Score: 0.1094, Precision: 0.2460, Recall: 0.0703\n",
      "Contamination: 0.02, F1 Score: 0.1663, Precision: 0.2286, Recall: 0.1307\n",
      "Contamination: 0.03, F1 Score: 0.1967, Precision: 0.2130, Recall: 0.1826\n",
      "Contamination: 0.04, F1 Score: 0.2082, Precision: 0.1952, Recall: 0.2232\n",
      "Contamination: 0.05, F1 Score: 0.2130, Precision: 0.1811, Recall: 0.2587\n",
      "Contamination: 0.06, F1 Score: 0.2135, Precision: 0.1690, Recall: 0.2898\n",
      "Contamination: 0.07, F1 Score: 0.2113, Precision: 0.1584, Recall: 0.3170\n",
      "Contamination: 0.08, F1 Score: 0.2086, Precision: 0.1499, Recall: 0.3428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.060000000000000005), np.float64(0.21352681118083286))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_optimal_contamination(df_pca)\n",
    "# This will print the F1 scores for different contamination levels and return the best one.\n",
    "# Note: The contamination level is crucial for the performance of the Isolation Forest model.\n",
    "# optimal contamination level is around 0.06 for our dataset (Contamination: 0.06, F1 Score: 0.2135, Precision: 0.1690, Recall: 0.2898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2473d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.7383\n"
     ]
    }
   ],
   "source": [
    "#Here we calculate the roc-auc to compare with other models\n",
    "from sklearn.metrics import roc_auc_score   \n",
    "roc_auc = roc_auc_score(results['is_fraud'], results['anomaly_likelihood'])\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
