{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0d2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb6d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive “day” from TransactionDT\n",
    "df[\"day\"] = (df[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "\n",
    "# derive card activation date from TransactionDT\n",
    "df['D1new'] = (df['TransactionDT'] // (60*60*24)) - df['D1'] + 2000\n",
    "\n",
    "# drop TransactionDT as it is no longer needed\n",
    "df.drop(\"TransactionDT\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a435cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3976e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify UIDs\n",
    "def compute_uids(df):\n",
    "\n",
    "    return (\n",
    "        df['card1'].astype(str) + \"_\" +\n",
    "        df['addr1'].astype(str) + \"_\" +\n",
    "        df['D1new'].astype(str) + \"_\" +\n",
    "        df['P_emaildomain'].astype(str) + \"_\" +\n",
    "        df['C1'].astype(str)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b01e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card1, card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ProductCD, Unique values: 5\n",
      "Column: card4, Unique values: 4\n",
      "Column: card6, Unique values: 4\n",
      "Column: P_emaildomain, Unique values: 59\n",
      "Column: R_emaildomain, Unique values: 60\n",
      "Column: M1, Unique values: 2\n",
      "Column: M2, Unique values: 2\n",
      "Column: M3, Unique values: 2\n",
      "Column: M4, Unique values: 3\n",
      "Column: M5, Unique values: 2\n",
      "Column: M6, Unique values: 2\n",
      "Column: M7, Unique values: 2\n",
      "Column: M8, Unique values: 2\n",
      "Column: M9, Unique values: 2\n",
      "Column: card1, Unique values: 13553\n",
      "Column: card2, Unique values: 500\n",
      "Column: card3, Unique values: 114\n",
      "Column: card5, Unique values: 119\n",
      "Column: addr1, Unique values: 332\n",
      "Column: addr2, Unique values: 74\n"
     ]
    }
   ],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones else drop them\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# extract high cardinality categorical columns\n",
    "high_card_cols = [c for c in cat_cols_rem if df[c].nunique() > 10]\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12be534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical columns: ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\n"
     ]
    }
   ],
   "source": [
    "print(f\"High cardinality categorical columns: {high_card_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>...</th>\n",
       "      <th>M1_T</th>\n",
       "      <th>M2_T</th>\n",
       "      <th>M3_T</th>\n",
       "      <th>M4_M1</th>\n",
       "      <th>M4_M2</th>\n",
       "      <th>M5_T</th>\n",
       "      <th>M6_T</th>\n",
       "      <th>M7_T</th>\n",
       "      <th>M8_T</th>\n",
       "      <th>M9_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 348 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionAmt  card1  card2  card3  card5  addr1  \\\n",
       "0        2987000        0            68.5  13926    NaN  150.0  142.0  315.0   \n",
       "1        2987001        0            29.0   2755  404.0  150.0  102.0  325.0   \n",
       "2        2987002        0            59.0   4663  490.0  150.0  166.0  330.0   \n",
       "3        2987003        0            50.0  18132  567.0  150.0  117.0  476.0   \n",
       "4        2987004        0            50.0   4497  514.0  150.0  102.0  420.0   \n",
       "\n",
       "   addr2  dist1  ... M1_T M2_T  M3_T  M4_M1  M4_M2  M5_T  M6_T  M7_T  M8_T  \\\n",
       "0   87.0   19.0  ...    1    1     1      0      1     0     1     0     0   \n",
       "1   87.0    8.0  ...    0    0     0      0      0     1     1     0     0   \n",
       "2   87.0  287.0  ...    1    1     1      0      0     0     0     0     0   \n",
       "3   87.0    8.0  ...    0    0     0      0      0     1     0     0     0   \n",
       "4   87.0    8.0  ...    0    0     0      0      0     0     0     0     0   \n",
       "\n",
       "   M9_T  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 348 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2c1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode high cardinality categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Store encoders for later inference\n",
    "encoders = {}\n",
    "for col in high_card_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b823102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all columns from high cardinality categorical columns\n",
    "other_cols = [c for c in df.columns if c not in high_card_cols]\n",
    "other_cols.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8af0b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 250514 normal UIDs → 454451 transactions\n",
      "CV set on 74791 UIDs → 136089 transactions\n"
     ]
    }
   ],
   "source": [
    "# Create training and CV sets by UID\n",
    "\n",
    "# Compute UIDs for every transaction\n",
    "df['UID'] = compute_uids(df)   # assumes compute_uids(df) returns a Series of length df\n",
    "\n",
    "# Label each UID as fraud if any transaction is fraudulent\n",
    "uid_labels = df.groupby('UID')['isFraud'].max().rename('UID_isFraud')\n",
    "df = df.merge(uid_labels, left_on='UID', right_index=True)\n",
    "\n",
    "# Split UIDs into “normal” vs. “fraud”\n",
    "normal_uids = uid_labels[uid_labels == 0].index\n",
    "fraud_uids  = uid_labels[uid_labels == 1].index\n",
    "\n",
    "# Hold out 20% of normal UIDs for CV:\n",
    "rng = np.random.RandomState(42)\n",
    "hold_normals = rng.choice(normal_uids, size=int(0.2 * len(normal_uids)), replace=False)\n",
    "\n",
    "# TRAIN on the remaining 80% normal UIDs\n",
    "train_uids = np.setdiff1d(normal_uids, hold_normals)\n",
    "train_df   = df[df['UID'].isin(train_uids)].copy()\n",
    "\n",
    "# CV on all fraud UIDs + held‐out normals\n",
    "cv_uids = np.concatenate([fraud_uids, hold_normals])\n",
    "cv_df   = df[df['UID'].isin(cv_uids)].copy()\n",
    "\n",
    "print(f\"Training on {len(train_uids)} normal UIDs → {train_df.shape[0]} transactions\")\n",
    "print(f\"CV set on {len(cv_uids)} UIDs → {cv_df.shape[0]} transactions\")\n",
    "\n",
    "# Build feature‐matrices (drop isFraud & UID)\n",
    "X_train = train_df.drop(['isFraud','UID'], axis=1)\n",
    "X_cv    = cv_df   .drop(['isFraud','UID'], axis=1)\n",
    "y_cv    = cv_df['isFraud'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b9801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal UIDs available: 313142\n",
      "Number of fraud UIDs available:  12163\n",
      "UIDs in training set:            250514\n",
      "UIDs in CV set:                  74791\n"
     ]
    }
   ],
   "source": [
    "# how many UIDs in each set?\n",
    "n_normal_uids = len(normal_uids)\n",
    "n_fraud_uids  = len(fraud_uids)\n",
    "n_train_uids  = train_df['UID'].nunique()\n",
    "n_cv_uids     = cv_df  ['UID'].nunique()\n",
    "\n",
    "print(f\"Number of normal UIDs available: {n_normal_uids}\")\n",
    "print(f\"Number of fraud UIDs available:  {n_fraud_uids}\")\n",
    "print(f\"UIDs in training set:            {n_train_uids}\")\n",
    "print(f\"UIDs in CV set:                  {n_cv_uids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "non_one_hot_cols = [col for col in X_train.columns if col not in one_hot_cols and col not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_cv_scaled    = X_cv.copy()\n",
    "\n",
    "X_train_scaled[non_one_hot_cols] = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_cv_scaled[non_one_hot_cols]    = scaler.transform(X_cv[non_one_hot_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_num_train = torch.tensor(X_train_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_train = torch.tensor(X_train_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_val = torch.tensor(X_cv_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_val = torch.tensor(X_cv_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "y_valid = torch.FloatTensor(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "911478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_num_train, X_cat_train, X_num_train, X_cat_train)\n",
    "val_ds   = TensorDataset(X_num_val, X_cat_val, X_num_val, X_cat_val)\n",
    "\n",
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3af693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UID‐aware DataLoader for CV aggregation\n",
    "from collections import defaultdict\n",
    "\n",
    "# Extract the unique CV UIDs and build a mapping → index\n",
    "unique_cv_uids = cv_df['UID'].unique()\n",
    "uid2idx        = {uid: i for i, uid in enumerate(unique_cv_uids)}\n",
    "\n",
    "# Map your UID column to these integer indices\n",
    "uid_idx_arr = cv_df['UID'].map(uid2idx).astype(int).values\n",
    "\n",
    "# Now you can build a LongTensor\n",
    "uid_cv_tensor = torch.tensor(uid_idx_arr, dtype=torch.long)\n",
    "\n",
    "# Create your dataset & loader as before\n",
    "uid_eval_ds     = TensorDataset(X_num_val, X_cat_val, uid_cv_tensor)\n",
    "uid_eval_loader = DataLoader(uid_eval_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73448abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2):\n",
    "        super(AutoEncoderWithEmbeddings, self).__init__()\n",
    "        \n",
    "        # Create embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, min(50, (num_categories + 1)//2))\n",
    "            for num_categories in cat_cardinalities\n",
    "        ])\n",
    "        \n",
    "        emb_size_total = sum([emb.embedding_dim for emb in self.embeddings])\n",
    "        total_input_size = num_numeric + emb_size_total\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(total_input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, hidden2)\n",
    "        self.fc5 = nn.Linear(hidden2, hidden1)\n",
    "        self.fc6 = nn.Linear(hidden1, total_input_size)\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Embed categorical variables\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embeds = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        # Concatenate numerical + embeddings\n",
    "        x = torch.cat([x_num, embeds], dim=1)\n",
    "\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a15fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe09b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9981a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numeric features (including one-hot encodings)\n",
    "num_numeric = len(other_cols)\n",
    "\n",
    "# Cardinalities (unique values) of each categorical feature\n",
    "cat_cardinalities = [df[col].nunique() for col in high_card_cols]\n",
    "\n",
    "# Initialize the embedding autoencoder model\n",
    "seed_everything(SEED)\n",
    "model = AutoEncoderWithEmbeddings(num_numeric=num_numeric, \n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            hidden1=128, hidden2=64, code_size=8, \n",
    "                            dropout_rate=0.2)\n",
    "\n",
    "losses = per_sample_mse(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float32(0.59493107), np.float32(0.5586943), np.float32(0.57017577), np.float32(0.57132477), np.float32(0.57208365)]\n",
      "1.454253\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66da7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,generator,criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        batch_losses.append(loss.item())\n",
    "    mean_loss = np.mean(batch_losses)    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04b30bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "    \n",
    "    def continue_training(self,current_score):\n",
    "        if self.best_score > current_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(\"New best score:\", current_score)\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(self.counter, \" iterations since best score.\")\n",
    "                \n",
    "        return self.counter <= self.patience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4465904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    #Setting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    \n",
    "    #Training loop\n",
    "    start_time=time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, y_true_full)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        #showing last training loss after each epoch\n",
    "        all_train_losses.append(np.mean(train_loss))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        #evaluating the model on the test set after each epoch    \n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.continue_training(valid_loss):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "    training_execution_time=time.time()-start_time\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d616349",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef8007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.7013870484724238\n",
      "valid loss: 1.0702895649841853\n",
      "New best score: 1.0702895649841853\n",
      "\n",
      "Epoch 1: train loss: 0.5656663658986757\n",
      "valid loss: 0.9581994126389798\n",
      "New best score: 0.9581994126389798\n",
      "\n",
      "Epoch 2: train loss: 0.4996548315828985\n",
      "valid loss: 0.8829813571576786\n",
      "New best score: 0.8829813571576786\n",
      "\n",
      "Epoch 3: train loss: 0.45372421274314056\n",
      "valid loss: 0.8172865604099474\n",
      "New best score: 0.8172865604099474\n",
      "\n",
      "Epoch 4: train loss: 0.4172271229326725\n",
      "valid loss: 0.777029902975362\n",
      "New best score: 0.777029902975362\n",
      "\n",
      "Epoch 5: train loss: 0.38867922513200354\n",
      "valid loss: 0.7506901402782676\n",
      "New best score: 0.7506901402782676\n",
      "\n",
      "Epoch 6: train loss: 0.3653688493209916\n",
      "valid loss: 0.7239140411628816\n",
      "New best score: 0.7239140411628816\n",
      "\n",
      "Epoch 7: train loss: 0.34508272077519075\n",
      "valid loss: 0.6989223052348409\n",
      "New best score: 0.6989223052348409\n",
      "\n",
      "Epoch 8: train loss: 0.3258373655278135\n",
      "valid loss: 0.6735067040960592\n",
      "New best score: 0.6735067040960592\n",
      "\n",
      "Epoch 9: train loss: 0.3099444522648244\n",
      "valid loss: 0.6523529481618924\n",
      "New best score: 0.6523529481618924\n",
      "\n",
      "Epoch 10: train loss: 0.2948665053222899\n",
      "valid loss: 0.6310995251948672\n",
      "New best score: 0.6310995251948672\n",
      "\n",
      "Epoch 11: train loss: 0.28221674238306443\n",
      "valid loss: 0.6114560692269999\n",
      "New best score: 0.6114560692269999\n",
      "\n",
      "Epoch 12: train loss: 0.2702590557536832\n",
      "valid loss: 0.5970554470567775\n",
      "New best score: 0.5970554470567775\n",
      "\n",
      "Epoch 13: train loss: 0.26068380991886325\n",
      "valid loss: 0.5825458990227907\n",
      "New best score: 0.5825458990227907\n",
      "\n",
      "Epoch 14: train loss: 0.25064368728917463\n",
      "valid loss: 0.5729370640408724\n",
      "New best score: 0.5729370640408724\n",
      "\n",
      "Epoch 15: train loss: 0.24168712471251017\n",
      "valid loss: 0.5534685824934701\n",
      "New best score: 0.5534685824934701\n",
      "\n",
      "Epoch 16: train loss: 0.2346448144363659\n",
      "valid loss: 0.5388959980100617\n",
      "New best score: 0.5388959980100617\n",
      "\n",
      "Epoch 17: train loss: 0.22673388721572385\n",
      "valid loss: 0.5276890917492092\n",
      "New best score: 0.5276890917492092\n",
      "\n",
      "Epoch 18: train loss: 0.2209547018581951\n",
      "valid loss: 0.5118781168265898\n",
      "New best score: 0.5118781168265898\n",
      "\n",
      "Epoch 19: train loss: 0.21572439419525163\n",
      "valid loss: 0.5059511430729601\n",
      "New best score: 0.5059511430729601\n",
      "\n",
      "Epoch 20: train loss: 0.2106087406140727\n",
      "valid loss: 0.4972396580628434\n",
      "New best score: 0.4972396580628434\n",
      "\n",
      "Epoch 21: train loss: 0.2064434049718149\n",
      "valid loss: 0.4845862470119071\n",
      "New best score: 0.4845862470119071\n",
      "\n",
      "Epoch 22: train loss: 0.20204144483851688\n",
      "valid loss: 0.4702978953999236\n",
      "New best score: 0.4702978953999236\n",
      "\n",
      "Epoch 23: train loss: 0.19905324280261993\n",
      "valid loss: 0.46182082551426457\n",
      "New best score: 0.46182082551426457\n",
      "\n",
      "Epoch 24: train loss: 0.19393130136771244\n",
      "valid loss: 0.4524643519487148\n",
      "New best score: 0.4524643519487148\n",
      "\n",
      "Epoch 25: train loss: 0.1918454992979228\n",
      "valid loss: 0.4470874988719037\n",
      "New best score: 0.4470874988719037\n",
      "\n",
      "Epoch 26: train loss: 0.19073766902055558\n",
      "valid loss: 0.43548666618596343\n",
      "New best score: 0.43548666618596343\n",
      "\n",
      "Epoch 27: train loss: 0.1866519061663935\n",
      "valid loss: 0.42890170687123347\n",
      "New best score: 0.42890170687123347\n",
      "\n",
      "Epoch 28: train loss: 0.18639254825911275\n",
      "valid loss: 0.4282481076573967\n",
      "New best score: 0.4282481076573967\n",
      "\n",
      "Epoch 29: train loss: 0.1836516012076859\n",
      "valid loss: 0.42825989441988166\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 30: train loss: 0.1802386240110741\n",
      "valid loss: 0.4167848228287876\n",
      "New best score: 0.4167848228287876\n",
      "\n",
      "Epoch 31: train loss: 0.18153109883906338\n",
      "valid loss: 0.4104661546591529\n",
      "New best score: 0.4104661546591529\n",
      "\n",
      "Epoch 32: train loss: 0.1786751509098722\n",
      "valid loss: 0.4087737672647139\n",
      "New best score: 0.4087737672647139\n",
      "\n",
      "Epoch 33: train loss: 0.17777658070160732\n",
      "valid loss: 0.40627871882310485\n",
      "New best score: 0.40627871882310485\n",
      "\n",
      "Epoch 34: train loss: 0.1759326182104446\n",
      "valid loss: 0.4046553796003188\n",
      "New best score: 0.4046553796003188\n",
      "\n",
      "Epoch 35: train loss: 0.17627674814414335\n",
      "valid loss: 0.40062444094092325\n",
      "New best score: 0.40062444094092325\n",
      "\n",
      "Epoch 36: train loss: 0.17379088720793393\n",
      "valid loss: 0.39920758089064656\n",
      "New best score: 0.39920758089064656\n",
      "\n",
      "Epoch 37: train loss: 0.17280115099909069\n",
      "valid loss: 0.3977690024819589\n",
      "New best score: 0.3977690024819589\n",
      "\n",
      "Epoch 38: train loss: 0.17134752875479223\n",
      "valid loss: 0.39344160213954465\n",
      "New best score: 0.39344160213954465\n",
      "\n",
      "Epoch 39: train loss: 0.16942517668310855\n",
      "valid loss: 0.39060150574248537\n",
      "New best score: 0.39060150574248537\n",
      "\n",
      "Epoch 40: train loss: 0.17014961464850753\n",
      "valid loss: 0.3896699966792774\n",
      "New best score: 0.3896699966792774\n",
      "\n",
      "Epoch 41: train loss: 0.167915853512314\n",
      "valid loss: 0.388108118444233\n",
      "New best score: 0.388108118444233\n",
      "\n",
      "Epoch 42: train loss: 0.16846449247787934\n",
      "valid loss: 0.3841587931365895\n",
      "New best score: 0.3841587931365895\n",
      "\n",
      "Epoch 43: train loss: 0.16631021649316624\n",
      "valid loss: 0.3835798070899078\n",
      "New best score: 0.3835798070899078\n",
      "\n",
      "Epoch 44: train loss: 0.16657504957874078\n",
      "valid loss: 0.3798002434292234\n",
      "New best score: 0.3798002434292234\n",
      "\n",
      "Epoch 45: train loss: 0.16632715728435968\n",
      "valid loss: 0.3843103847501421\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 46: train loss: 0.16297538126273467\n",
      "valid loss: 0.3797499271375792\n",
      "New best score: 0.3797499271375792\n",
      "\n",
      "Epoch 47: train loss: 0.16408896576277576\n",
      "valid loss: 0.38203218332806926\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 48: train loss: 0.16424952397911666\n",
      "valid loss: 0.3763603405060625\n",
      "New best score: 0.3763603405060625\n",
      "\n",
      "Epoch 49: train loss: 0.16157490058659432\n",
      "valid loss: 0.3783434387996681\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 50: train loss: 0.16251760539976326\n",
      "valid loss: 0.3750265124428989\n",
      "New best score: 0.3750265124428989\n",
      "\n",
      "Epoch 51: train loss: 0.16168167303643516\n",
      "valid loss: 0.37665347256382603\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 52: train loss: 0.16194627140355003\n",
      "valid loss: 0.3714735396766573\n",
      "New best score: 0.3714735396766573\n",
      "\n",
      "Epoch 53: train loss: 0.16089064735281575\n",
      "valid loss: 0.36977121673692437\n",
      "New best score: 0.36977121673692437\n",
      "\n",
      "Epoch 54: train loss: 0.1617081625856929\n",
      "valid loss: 0.37085041893940224\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 55: train loss: 0.1597095985250833\n",
      "valid loss: 0.3686234953820257\n",
      "New best score: 0.3686234953820257\n",
      "\n",
      "Epoch 56: train loss: 0.15880861010838737\n",
      "valid loss: 0.37114871948733363\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 57: train loss: 0.15841808334529936\n",
      "valid loss: 0.367305377036109\n",
      "New best score: 0.367305377036109\n",
      "\n",
      "Epoch 58: train loss: 0.15955366904906057\n",
      "valid loss: 0.36761486365046714\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 59: train loss: 0.15668609086424112\n",
      "valid loss: 0.3657358172244595\n",
      "New best score: 0.3657358172244595\n",
      "\n",
      "Epoch 60: train loss: 0.15927634440228208\n",
      "valid loss: 0.3668263613953626\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 61: train loss: 0.15852559781591366\n",
      "valid loss: 0.36349872768597496\n",
      "New best score: 0.36349872768597496\n",
      "\n",
      "Epoch 62: train loss: 0.1579102338417559\n",
      "valid loss: 0.36609847821239244\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 63: train loss: 0.15539276827260987\n",
      "valid loss: 0.36448873739157406\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 64: train loss: 0.1563151590274395\n",
      "valid loss: 0.35912415903425754\n",
      "New best score: 0.35912415903425754\n",
      "\n",
      "Epoch 65: train loss: 0.15502306167036295\n",
      "valid loss: 0.35830379549161834\n",
      "New best score: 0.35830379549161834\n",
      "\n",
      "Epoch 66: train loss: 0.1547440016927483\n",
      "valid loss: 0.35794572915909884\n",
      "New best score: 0.35794572915909884\n",
      "\n",
      "Epoch 67: train loss: 0.15515783836914077\n",
      "valid loss: 0.35558178897638965\n",
      "New best score: 0.35558178897638965\n",
      "\n",
      "Epoch 68: train loss: 0.15429485964311943\n",
      "valid loss: 0.3570451599202658\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 69: train loss: 0.15423269380253177\n",
      "valid loss: 0.3559945188462734\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 70: train loss: 0.15510986256203405\n",
      "valid loss: 0.3542931801114315\n",
      "New best score: 0.3542931801114315\n",
      "\n",
      "Epoch 71: train loss: 0.15318474655987713\n",
      "valid loss: 0.3543273416957013\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 72: train loss: 0.15333943001858824\n",
      "valid loss: 0.35610985212532204\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 73: train loss: 0.15373358985425922\n",
      "valid loss: 0.353891851422482\n",
      "New best score: 0.353891851422482\n",
      "\n",
      "Epoch 74: train loss: 0.15215997686525723\n",
      "valid loss: 0.35213432133309824\n",
      "New best score: 0.35213432133309824\n",
      "\n",
      "Epoch 75: train loss: 0.15314336851876867\n",
      "valid loss: 0.35918447529350905\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 76: train loss: 0.15205198746208134\n",
      "valid loss: 0.3529867360130289\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 77: train loss: 0.15274133850392457\n",
      "valid loss: 0.35117644721404057\n",
      "New best score: 0.35117644721404057\n",
      "\n",
      "Epoch 78: train loss: 0.15217716582455076\n",
      "valid loss: 0.3521197145258574\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 79: train loss: 0.15073562243131097\n",
      "valid loss: 0.35193938120527374\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 80: train loss: 0.1518627487475405\n",
      "valid loss: 0.35136589402318896\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 81: train loss: 0.153478542485417\n",
      "valid loss: 0.3505557580549914\n",
      "New best score: 0.3505557580549914\n",
      "\n",
      "Epoch 82: train loss: 0.14977849095321452\n",
      "valid loss: 0.34869871577037903\n",
      "New best score: 0.34869871577037903\n",
      "\n",
      "Epoch 83: train loss: 0.150777434233811\n",
      "valid loss: 0.3484696039747923\n",
      "New best score: 0.3484696039747923\n",
      "\n",
      "Epoch 84: train loss: 0.14911410736071096\n",
      "valid loss: 0.34806082044777115\n",
      "New best score: 0.34806082044777115\n",
      "\n",
      "Epoch 85: train loss: 0.15011272217809885\n",
      "valid loss: 0.34660698711535987\n",
      "New best score: 0.34660698711535987\n",
      "\n",
      "Epoch 86: train loss: 0.15028171077672695\n",
      "valid loss: 0.3464629844223198\n",
      "New best score: 0.3464629844223198\n",
      "\n",
      "Epoch 87: train loss: 0.1498606627689557\n",
      "valid loss: 0.34569376796708073\n",
      "New best score: 0.34569376796708073\n",
      "\n",
      "Epoch 88: train loss: 0.14969765626498172\n",
      "valid loss: 0.3454648138941231\n",
      "New best score: 0.3454648138941231\n",
      "\n",
      "Epoch 89: train loss: 0.1495042281091079\n",
      "valid loss: 0.34549824226843684\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 90: train loss: 0.14930365279081975\n",
      "valid loss: 0.3464671852333205\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 91: train loss: 0.14846796143028113\n",
      "valid loss: 0.3426679358222431\n",
      "New best score: 0.3426679358222431\n",
      "\n",
      "Epoch 92: train loss: 0.14962958260061773\n",
      "valid loss: 0.3465672631404902\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 93: train loss: 0.14898265033188435\n",
      "valid loss: 0.3425567725715332\n",
      "New best score: 0.3425567725715332\n",
      "\n",
      "Epoch 94: train loss: 0.14840827956963498\n",
      "valid loss: 0.3423399755213046\n",
      "New best score: 0.3423399755213046\n",
      "\n",
      "Epoch 95: train loss: 0.14683014363352512\n",
      "valid loss: 0.34179219205800754\n",
      "New best score: 0.34179219205800754\n",
      "\n",
      "Epoch 96: train loss: 0.14573314128997358\n",
      "valid loss: 0.34211800717993784\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 97: train loss: 0.14620330998556572\n",
      "valid loss: 0.3426636415428685\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 98: train loss: 0.14590364059450123\n",
      "valid loss: 0.33904502165496797\n",
      "New best score: 0.33904502165496797\n",
      "\n",
      "Epoch 99: train loss: 0.14584486509530661\n",
      "valid loss: 0.3389723961402599\n",
      "New best score: 0.3389723961402599\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,train_loader,val_loader,optimizer,criterion,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9684c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UID-level reconstruction errors\n",
    "def compute_uid_errors(model, loader, idx2uid):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction='none')\n",
    "    uid_errors = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, uid_idxs in loader:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "            # rebuild full “true” vector\n",
    "            true_embeds = [model.embeddings[i](xb_cat[:, i])\n",
    "                           for i in range(len(model.embeddings))]\n",
    "            y_true_full = torch.cat([xb_num] + true_embeds, dim=1)\n",
    "\n",
    "            losses = criterion(y_pred, y_true_full).mean(dim=1).cpu().numpy()\n",
    "            for idx, err in zip(uid_idxs.cpu().numpy(), losses):\n",
    "                actual_uid = idx2uid[idx]\n",
    "                uid_errors[actual_uid].append(err)\n",
    "\n",
    "    return uid_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b21a2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC: 0.7791\n"
     ]
    }
   ],
   "source": [
    "# evaluate UID‐level reconstruction errors\n",
    "\n",
    "idx2uid    = list(unique_cv_uids)\n",
    "uid_errors = compute_uid_errors(model, uid_eval_loader, idx2uid)\n",
    "\n",
    "# average per‐UID\n",
    "uid_avg = {u: np.mean(errs) for u, errs in uid_errors.items()}\n",
    "uid_df  = pd.DataFrame.from_dict(uid_avg, orient='index', \n",
    "                                  columns=['avg_error'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "auc = roc_auc_score(uid_df['true_label'], uid_df['avg_error'])\n",
    "print(f\"UID‐level ROC AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d66bc6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 70th percentile train-error threshold = 0.6999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "train_errs = per_sample_mse(model, train_loader)\n",
    "thresh     = np.percentile(train_errs, 70)  # 70th percentile as threshold\n",
    "uid_df['pred'] = (uid_df['avg_error'] >= thresh).astype(int)\n",
    "recall   = recall_score(uid_df['true_label'], uid_df['pred'])    # binary‐class recall\n",
    "print(f\"Recall at 70th percentile train-error threshold = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc880a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC (fraction rule):       0.7253\n",
      "UID‐level recall (majority‐vote rule): 0.7028\n"
     ]
    }
   ],
   "source": [
    "# UID‐level evaluation via fraction‐rule\n",
    "\n",
    "# Recompute uid_errors if needed\n",
    "idx2uid    = list(unique_cv_uids)\n",
    "uid_errors = compute_uid_errors(model, uid_eval_loader, idx2uid)\n",
    "\n",
    "# Build train‐error threshold (70th pct of per‐sample errs)\n",
    "train_errs = per_sample_mse(model, train_loader)\n",
    "thresh     = np.percentile(train_errs, 70)  # 70th percentile as threshold\n",
    "\n",
    "# For each UID, compute fraction of its txns flagged as fraud\n",
    "uid_frac = {\n",
    "    uid: np.mean(np.array(errs) >= thresh)\n",
    "    for uid, errs in uid_errors.items()\n",
    "}\n",
    "\n",
    "# Build DataFrame and true labels\n",
    "uid_df = pd.DataFrame.from_dict(uid_frac, orient='index',\n",
    "                                columns=['fraud_frac'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# Classify UID as fraud if majority of its txns exceed threshold\n",
    "uid_df['pred'] = (uid_df['fraud_frac'] >= 0.50).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc_frac = roc_auc_score(uid_df['true_label'], uid_df['fraud_frac'])\n",
    "recall_frac = recall_score(uid_df['true_label'], uid_df['pred'])\n",
    "\n",
    "print(f\"UID‐level ROC AUC (fraction rule):       {auc_frac:.4f}\")\n",
    "print(f\"UID‐level recall (majority‐vote rule): {recall_frac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
