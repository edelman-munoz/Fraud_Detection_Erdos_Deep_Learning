{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Utils\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a2cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Update these paths if running on a different machine.\n",
    "txn = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_transaction.csv\")\n",
    "idm = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_identity.csv\")\n",
    "\n",
    "# Derive simple features (day, D1new) and drop TransactionDT\n",
    "txn[\"day\"] = (txn[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "txn[\"D1new\"] = (txn[\"TransactionDT\"] // (60*60*24)) - txn[\"D1\"] + 2000\n",
    "txn.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# Merge identity into transactions, drop TransactionID afterward\n",
    "df = txn.merge(idm, on=\"TransactionID\", how=\"left\")\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657089dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UID construction helpers\n",
    "def safe_str(series):\n",
    "    \"\"\"Convert to string with an explicit missing token to avoid 'nan' ambiguity.\"\"\"\n",
    "    return series.astype(\"object\").where(~series.isna(), \"__MISSING__\").astype(str)\n",
    "\n",
    "def compute_uids(df):\n",
    "    \"\"\"Stable UID built from card/address/email/C1 with explicit missing token.\"\"\"\n",
    "    return (\n",
    "        safe_str(df['card1']) + \"_\" +\n",
    "        safe_str(df['addr1']) + \"_\" +\n",
    "        safe_str(df['D1new']) + \"_\" +\n",
    "        safe_str(df['P_emaildomain']) + \"_\" +\n",
    "        safe_str(df['C1'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39440759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id_12, Unique values: 2\n",
      "One-hot encoding id_12 with 2 unique values\n",
      "Column: id_13, Unique values: 54\n",
      "Column: id_15, Unique values: 3\n",
      "One-hot encoding id_15 with 3 unique values\n",
      "Column: id_16, Unique values: 2\n",
      "One-hot encoding id_16 with 2 unique values\n",
      "Column: id_17, Unique values: 104\n",
      "Column: id_19, Unique values: 522\n",
      "Column: id_20, Unique values: 394\n",
      "Column: id_28, Unique values: 2\n",
      "One-hot encoding id_28 with 2 unique values\n",
      "Column: id_29, Unique values: 2\n",
      "One-hot encoding id_29 with 2 unique values\n",
      "Column: id_31, Unique values: 130\n",
      "Column: id_35, Unique values: 2\n",
      "One-hot encoding id_35 with 2 unique values\n",
      "Column: id_36, Unique values: 2\n",
      "One-hot encoding id_36 with 2 unique values\n",
      "Column: id_37, Unique values: 2\n",
      "One-hot encoding id_37 with 2 unique values\n",
      "Column: id_38, Unique values: 2\n",
      "One-hot encoding id_38 with 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# categorical ID columns\n",
    "id_cols = [c for c in df.columns if c.startswith(\"id_\")]\n",
    "\n",
    "# but id_01 to id_11 are numerical so need to exclude them\n",
    "id_cat_cols = [c for c in id_cols if not c.startswith(\"id_0\")]\n",
    "id_cat_cols.remove(\"id_11\")  # id_11 is a numerical column\n",
    "\n",
    "# extract high cardinality categorical ID columns\n",
    "id_high_card_cols = [c for c in id_cat_cols if df[c].nunique() > 10]\n",
    "\n",
    "# one-hot encode categorical features with low cardinality\n",
    "for c in id_cat_cols:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2549cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs to be imputed: ['V230', 'V199', 'V281', 'V34', 'V45', 'V18', 'V81', 'V190', 'V237', 'V319', 'V72', 'V113', 'V197', 'V7', 'V65', 'V168', 'V213', 'V260', 'V214', 'V68', 'V92', 'V289', 'id_20', 'V229', 'V15', 'V29', 'V134', 'V246', 'V96', 'V305', 'V117', 'V252', 'V182', 'V126', 'V52', 'V292', 'V204', 'V311', 'V136', 'V98', 'V4', 'D1', 'V3', 'V79', 'V50', 'V59', 'V114', 'V247', 'V170', 'V312', 'V266', 'V119', 'V62', 'V1', 'V135', 'V206', 'V11', 'V26', 'V254', 'V30', 'V94', 'D10', 'V265', 'V310', 'V278', 'V216', 'V116', 'V301', 'V110', 'V186', 'V133', 'V2', 'V185', 'V271', 'V123', 'V198', 'V33', 'V36', 'V194', 'V178', 'V231', 'V243', 'V287', 'V104', 'V192', 'D1new', 'V124', 'V195', 'id_05', 'V71', 'V181', 'V275', 'V302', 'V128', 'V55', 'V290', 'V102', 'V320', 'V131', 'V202', 'V226', 'V53', 'V276', 'V37', 'V9', 'V19', 'V70', 'V215', 'V82', 'V111', 'V283', 'V238', 'V201', 'V108', 'V205', 'V47', 'V85', 'D3', 'dist1', 'V183', 'V209', 'V63', 'V107', 'V73', 'V232', 'V46', 'V303', 'V6', 'V207', 'V296', 'V112', 'V306', 'V304', 'V21', 'V227', 'V67', 'V105', 'V300', 'V280', 'id_02', 'V188', 'V293', 'V103', 'V218', 'V321', 'V41', 'V298', 'V173', 'V90', 'V200', 'V263', 'V12', 'V14', 'V106', 'V97', 'V91', 'V129', 'V208', 'V212', 'V223', 'V269', 'V258', 'V244', 'V86', 'V48', 'V262', 'D15', 'V179', 'V32', 'V184', 'V16', 'V132', 'V277', 'V28', 'V297', 'V228', 'V267', 'V217', 'V222', 'V242', 'V42', 'V203', 'V88', 'V77', 'V177', 'V78', 'V38', 'D2', 'V20', 'V22', 'V264', 'V225', 'V137', 'V187', 'V253', 'V174', 'V57', 'V17', 'V115', 'V248', 'V56', 'V44', 'V295', 'V89', 'V120', 'V294', 'V171', 'V221', 'V167', 'V75', 'D11', 'V109', 'V61', 'V5', 'V284', 'V224', 'V40', 'V251', 'V180', 'id_01', 'V172', 'V74', 'V288', 'V236', 'V23', 'V25', 'V54', 'V233', 'V125', 'V273', 'V315', 'V69', 'V240', 'V256', 'V308', 'V259', 'V210', 'V317', 'V58', 'V257', 'V83', 'V220', 'V279', 'V245', 'V307', 'V241', 'V250', 'id_19', 'V10', 'V285', 'V39', 'V95', 'V76', 'V80', 'V299', 'V318', 'V49', 'V249', 'V43', 'V196', 'V99', 'V270', 'V239', 'id_11', 'V13', 'V234', 'V169', 'V60', 'V282', 'V286', 'V64', 'V121', 'V24', 'V261', 'V272', 'V84', 'V274', 'V100', 'V313', 'D4', 'V8', 'D5', 'V268', 'V27', 'V51', 'V35', 'V175', 'V193', 'V127', 'V291', 'V309', 'V235', 'V189', 'V314', 'V316', 'V130', 'V219', 'V87', 'id_13', 'V101', 'V191', 'V93', 'V118', 'V122', 'V31', 'V176', 'V66', 'id_06', 'id_17', 'V211', 'V255']\n"
     ]
    }
   ],
   "source": [
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# include id_01 to id_11 as they are numerical\n",
    "id_num_cols = [c for c in id_cols if c not in id_cat_cols]\n",
    "num_cols.extend(id_num_cols)\n",
    "\n",
    "# remove duplicates from the list\n",
    "num_cols = list(set(num_cols))\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card1, card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "print(f\"Columns with NaNs to be imputed: {nan_cols}\")\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: card4, Unique values: 4\n",
      "One-hot encoding card4 with 4 unique values\n",
      "Column: M2, Unique values: 2\n",
      "One-hot encoding M2 with 2 unique values\n",
      "Column: R_emaildomain, Unique values: 60\n",
      "Column: ProductCD, Unique values: 5\n",
      "One-hot encoding ProductCD with 5 unique values\n",
      "Column: id_31, Unique values: 130\n",
      "Column: M7, Unique values: 2\n",
      "One-hot encoding M7 with 2 unique values\n",
      "Column: addr1, Unique values: 332\n",
      "Column: M4, Unique values: 3\n",
      "One-hot encoding M4 with 3 unique values\n",
      "Column: M5, Unique values: 2\n",
      "One-hot encoding M5 with 2 unique values\n",
      "Column: DeviceType, Unique values: 2\n",
      "One-hot encoding DeviceType with 2 unique values\n",
      "Column: M9, Unique values: 2\n",
      "One-hot encoding M9 with 2 unique values\n",
      "Column: card2, Unique values: 500\n",
      "Column: DeviceInfo, Unique values: 1786\n",
      "Column: card1, Unique values: 13553\n",
      "Column: M8, Unique values: 2\n",
      "One-hot encoding M8 with 2 unique values\n",
      "Column: card3, Unique values: 114\n",
      "Column: addr2, Unique values: 74\n",
      "Column: M3, Unique values: 2\n",
      "One-hot encoding M3 with 2 unique values\n",
      "Column: card6, Unique values: 4\n",
      "One-hot encoding card6 with 4 unique values\n",
      "Column: M1, Unique values: 2\n",
      "One-hot encoding M1 with 2 unique values\n",
      "Column: P_emaildomain, Unique values: 59\n",
      "Column: card5, Unique values: 119\n",
      "Column: M6, Unique values: 2\n",
      "One-hot encoding M6 with 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# remove duplicates from the list\n",
    "cat_cols_rem = list(set(cat_cols_rem))\n",
    "\n",
    "# extract high cardinality categorical columns\n",
    "high_card_cols = [c for c in cat_cols_rem if df[c].nunique() > 10]\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12be534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical columns: ['card2', 'DeviceInfo', 'card3', 'addr2', 'id_13', 'addr1', 'card1', 'P_emaildomain', 'R_emaildomain', 'card5', 'id_20', 'id_17', 'id_19', 'id_31']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# include the high cardinality categorical ID columns but remove duplicates\n",
    "high_card_cols = list(set(high_card_cols + id_high_card_cols))  # remove duplicates\n",
    "print(f\"High cardinality categorical columns: {high_card_cols}\")\n",
    "print(len(high_card_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>M5_T</th>\n",
       "      <th>DeviceType_mobile</th>\n",
       "      <th>M9_T</th>\n",
       "      <th>M8_T</th>\n",
       "      <th>M3_T</th>\n",
       "      <th>card6_credit</th>\n",
       "      <th>card6_debit</th>\n",
       "      <th>card6_debit or credit</th>\n",
       "      <th>M1_T</th>\n",
       "      <th>M6_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  card1  card2  card3  card5  addr1  addr2  dist1  \\\n",
       "0        0            68.5  13926    NaN  150.0  142.0  315.0   87.0   19.0   \n",
       "1        0            29.0   2755  404.0  150.0  102.0  325.0   87.0    8.0   \n",
       "2        0            59.0   4663  490.0  150.0  166.0  330.0   87.0  287.0   \n",
       "3        0            50.0  18132  567.0  150.0  117.0  476.0   87.0    8.0   \n",
       "4        0            50.0   4497  514.0  150.0  102.0  420.0   87.0    8.0   \n",
       "\n",
       "  P_emaildomain  ... M5_T  DeviceType_mobile  M9_T  M8_T  M3_T  card6_credit  \\\n",
       "0           NaN  ...    0                  0     0     0     1             1   \n",
       "1     gmail.com  ...    1                  0     0     0     0             1   \n",
       "2   outlook.com  ...    0                  0     0     0     1             0   \n",
       "3     yahoo.com  ...    1                  0     0     0     0             0   \n",
       "4     gmail.com  ...    0                  1     0     0     0             1   \n",
       "\n",
       "   card6_debit  card6_debit or credit  M1_T  M6_T  \n",
       "0            0                      0     1     1  \n",
       "1            0                      0     0     1  \n",
       "2            1                      0     1     0  \n",
       "3            1                      0     0     0  \n",
       "4            0                      0     0     0  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2c1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode high cardinality categorical columns and\n",
    "# Store encoders for later inference\n",
    "encoders = {}\n",
    "for col in high_card_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b823102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all columns from high cardinality categorical columns\n",
    "other_cols = [c for c in df.columns if c not in high_card_cols]\n",
    "other_cols.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757705cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(normals) UIDs: 187885  | rows: 340202\n",
      "Val(normals)   UIDs: 62628    | rows: 113875\n",
      "Dev(mix)       UIDs: 37395    | rows: 68652\n",
      "Test(mix)      UIDs: 37397   | rows: 67811\n"
     ]
    }
   ],
   "source": [
    "# Faster way to split UIDs into train/val/dev/test sets\n",
    "# Create UID column using the robust helper\n",
    "df['UID'] = compute_uids(df)\n",
    "\n",
    "# Label UIDs as fraud if any txn is fraud, then split\n",
    "uid_labels = df.groupby('UID')['isFraud'].max().rename('UID_isFraud')\n",
    "df = df.merge(uid_labels, left_on='UID', right_index=True)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "normal_uids = uid_labels[uid_labels == 0].index.values\n",
    "fraud_uids  = uid_labels[uid_labels == 1].index.values\n",
    "\n",
    "rng.shuffle(normal_uids)\n",
    "n_norm = len(normal_uids)\n",
    "n_train = int(0.60 * n_norm)\n",
    "n_val   = int(0.20 * n_norm)\n",
    "\n",
    "train_norm_uids = normal_uids[:n_train]\n",
    "val_norm_uids   = normal_uids[n_train:n_train+n_val]\n",
    "hold_norm_uids  = normal_uids[n_train+n_val:]\n",
    "\n",
    "rng.shuffle(fraud_uids)\n",
    "n_fraud = len(fraud_uids)\n",
    "n_fraud_dev = n_fraud // 2\n",
    "dev_fraud_uids  = fraud_uids[:n_fraud_dev]\n",
    "test_fraud_uids = fraud_uids[n_fraud_dev:]\n",
    "\n",
    "n_hold = len(hold_norm_uids)\n",
    "n_hold_dev = n_hold // 2\n",
    "dev_norm_uids  = hold_norm_uids[:n_hold_dev]\n",
    "test_norm_uids = hold_norm_uids[n_hold_dev:]\n",
    "\n",
    "train_uids = set(train_norm_uids)\n",
    "val_uids   = set(val_norm_uids)\n",
    "dev_uids   = set(dev_norm_uids) | set(dev_fraud_uids)\n",
    "test_uids  = set(test_norm_uids) | set(test_fraud_uids)\n",
    "\n",
    "def slice_by_uids(df_in, uids):\n",
    "    return df_in[df_in['UID'].isin(uids)].copy()\n",
    "\n",
    "train_df = slice_by_uids(df, train_uids)  # normals only\n",
    "val_df   = slice_by_uids(df, val_uids)    # normals only\n",
    "dev_df   = slice_by_uids(df, dev_uids)    # mixture\n",
    "test_df  = slice_by_uids(df, test_uids)   # mixture\n",
    "\n",
    "print(f\"Train(normals) UIDs: {len(train_uids)}  | rows: {train_df.shape[0]}\")\n",
    "print(f\"Val(normals)   UIDs: {len(val_uids)}    | rows: {val_df.shape[0]}\")\n",
    "print(f\"Dev(mix)       UIDs: {len(dev_uids)}    | rows: {dev_df.shape[0]}\")\n",
    "print(f\"Test(mix)      UIDs: {len(test_uids)}   | rows: {test_df.shape[0]}\")\n",
    "\n",
    "# Feature matrices per split (drop label & UID)\n",
    "X_train = train_df.drop(['isFraud','UID'], axis=1)\n",
    "X_val   = val_df  .drop(['isFraud','UID'], axis=1)\n",
    "X_dev   = dev_df  .drop(['isFraud','UID'], axis=1)\n",
    "X_test  = test_df .drop(['isFraud','UID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "\n",
    "# Define columns to be standardized\n",
    "std_cols = [col for col in X_train.columns if col not in one_hot_cols and col not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled    = X_val.copy()\n",
    "X_dev_scaled    = X_dev.copy()\n",
    "X_test_scaled   = X_test.copy()\n",
    "\n",
    "X_train_scaled[std_cols] = scaler.fit_transform(X_train[std_cols])\n",
    "X_val_scaled[std_cols]   = scaler.transform(X_val[std_cols])\n",
    "X_dev_scaled[std_cols]   = scaler.transform(X_dev[std_cols])\n",
    "X_test_scaled[std_cols]  = scaler.transform(X_test[std_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e6f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_num_train = torch.tensor(X_train_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_train = torch.tensor(X_train_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_val = torch.tensor(X_val_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_val = torch.tensor(X_val_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_dev = torch.tensor(X_dev_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_dev = torch.tensor(X_dev_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_test = torch.tensor(X_test_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_test = torch.tensor(X_test_scaled[high_card_cols].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for train/val/dev/test sets\n",
    "train_ds = TensorDataset(X_num_train, X_cat_train, X_num_train, X_cat_train)\n",
    "val_ds   = TensorDataset(X_num_val, X_cat_val, X_num_val, X_cat_val)\n",
    "dev_ds   = TensorDataset(X_num_dev, X_cat_dev, X_num_dev, X_cat_dev)\n",
    "test_ds  = TensorDataset(X_num_test, X_cat_test, X_num_test, X_cat_test)\n",
    "\n",
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73448abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition with categorical embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2):\n",
    "        super(AutoEncoderWithEmbeddings, self).__init__()\n",
    "        \n",
    "        # Create embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, min(50, (num_categories + 1)//2))\n",
    "            for num_categories in cat_cardinalities\n",
    "        ])\n",
    "        \n",
    "        emb_size_total = sum([emb.embedding_dim for emb in self.embeddings])\n",
    "        total_input_size = num_numeric + emb_size_total\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(total_input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, hidden2)\n",
    "        self.fc5 = nn.Linear(hidden2, hidden1)\n",
    "        self.fc6 = nn.Linear(hidden1, total_input_size)\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Embed categorical variables\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embeds = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        # Concatenate numerical + embeddings\n",
    "        x = torch.cat([x_num, embeds], dim=1)\n",
    "\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee4103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def per_sample_mse(model, generator):\n",
    "    # Targets are built from *detached* embedding lookups to avoid target leakage\n",
    "    model.eval()\n",
    "    crit = torch.nn.MSELoss(reduction=\"none\")\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = []\n",
    "            if xb_cat.shape[1] > 0:\n",
    "                for i in range(len(model.embeddings)):\n",
    "                    true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "\n",
    "            # Compute Loss\n",
    "            l = crit(y_pred, y_true_full).mean(dim=1).cpu().numpy()\n",
    "            losses.extend(l.tolist())\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e35e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model,generator,criterion):\n",
    "    # Detached embedding targets for validation loss\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = []\n",
    "            if xb_cat.shape[1] > 0:\n",
    "                for i in range(len(model.embeddings)):\n",
    "                    true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, y_true_full).mean(dim=1)\n",
    "            batch_losses.extend(list(loss.cpu().numpy()))\n",
    "    return float(np.mean(batch_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649b1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,\n",
    "                  max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    # Train with detached embedding targets; early stop on Val(normals) and restore best checkpoint\n",
    "    model.train()\n",
    "\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=3, verbose=False):\n",
    "            self.patience = patience\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = np.inf\n",
    "            self.best_state = None\n",
    "        def step(self, current_score, model):\n",
    "            if current_score < self.best_score:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "                self.best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "                if self.verbose:\n",
    "                    print(\"New best score:\", current_score)\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f\"No improvement. Patience {self.counter}/{self.patience}\")\n",
    "            return self.counter < self.patience\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    start_time=time.time()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "            # detach embedding targets\n",
    "            with torch.no_grad():\n",
    "                true_embeds = []\n",
    "                if xb_cat.shape[1] > 0:\n",
    "                    for i in range(len(model.embeddings)):\n",
    "                        true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "                y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "            loss = criterion(y_pred, y_true_full).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.step(valid_loss, model):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    training_execution_time=time.time()-start_time\n",
    "    if apply_early_stopping and early_stopping.best_state is not None:\n",
    "        model.load_state_dict(early_stopping.best_state)\n",
    "\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9981a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numeric features (including one-hot encodings)\n",
    "num_numeric = len(other_cols)\n",
    "\n",
    "# Cardinalities of only high cardinality categorical features\n",
    "cat_cardinalities = [df[col].nunique() for col in high_card_cols]\n",
    "\n",
    "# Initialize the embedding autoencoder model\n",
    "seed_everything(SEED)\n",
    "model = AutoEncoderWithEmbeddings(num_numeric=num_numeric, \n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2)\n",
    "\n",
    "# specify loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Generate losses before training\n",
    "losses = per_sample_mse(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71341252 0.69325203 0.83842689 0.67517078 2.73102283]\n",
      "0.9292799331139524\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65450ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.6431472738434497\n",
      "valid loss: 0.5161669254302979\n",
      "New best score: 0.5161669254302979\n",
      "\n",
      "Epoch 1: train loss: 0.5215920304893551\n",
      "valid loss: 0.4903077781200409\n",
      "New best score: 0.4903077781200409\n",
      "\n",
      "Epoch 2: train loss: 0.49523935923002715\n",
      "valid loss: 0.47263652086257935\n",
      "New best score: 0.47263652086257935\n",
      "\n",
      "Epoch 3: train loss: 0.47577771248674033\n",
      "valid loss: 0.45923545956611633\n",
      "New best score: 0.45923545956611633\n",
      "\n",
      "Epoch 4: train loss: 0.46141232180416136\n",
      "valid loss: 0.4439484179019928\n",
      "New best score: 0.4439484179019928\n",
      "\n",
      "Epoch 5: train loss: 0.4467946750777108\n",
      "valid loss: 0.42838194966316223\n",
      "New best score: 0.42838194966316223\n",
      "\n",
      "Epoch 6: train loss: 0.4344280073517247\n",
      "valid loss: 0.4179028570652008\n",
      "New best score: 0.4179028570652008\n",
      "\n",
      "Epoch 7: train loss: 0.424805265037637\n",
      "valid loss: 0.40913650393486023\n",
      "New best score: 0.40913650393486023\n",
      "\n",
      "Epoch 8: train loss: 0.4181599153611893\n",
      "valid loss: 0.4034082293510437\n",
      "New best score: 0.4034082293510437\n",
      "\n",
      "Epoch 9: train loss: 0.41368859943590663\n",
      "valid loss: 0.399599552154541\n",
      "New best score: 0.399599552154541\n",
      "\n",
      "Epoch 10: train loss: 0.4097723707220608\n",
      "valid loss: 0.39590632915496826\n",
      "New best score: 0.39590632915496826\n",
      "\n",
      "Epoch 11: train loss: 0.4075493851102384\n",
      "valid loss: 0.3924192786216736\n",
      "New best score: 0.3924192786216736\n",
      "\n",
      "Epoch 12: train loss: 0.4031791995342513\n",
      "valid loss: 0.3891833424568176\n",
      "New best score: 0.3891833424568176\n",
      "\n",
      "Epoch 13: train loss: 0.4008666929445769\n",
      "valid loss: 0.38597095012664795\n",
      "New best score: 0.38597095012664795\n",
      "\n",
      "Epoch 14: train loss: 0.3987864380940459\n",
      "valid loss: 0.38262057304382324\n",
      "New best score: 0.38262057304382324\n",
      "\n",
      "Epoch 15: train loss: 0.3963581510952541\n",
      "valid loss: 0.38031837344169617\n",
      "New best score: 0.38031837344169617\n",
      "\n",
      "Epoch 16: train loss: 0.39375637975850497\n",
      "valid loss: 0.37775224447250366\n",
      "New best score: 0.37775224447250366\n",
      "\n",
      "Epoch 17: train loss: 0.3922663376743632\n",
      "valid loss: 0.37524211406707764\n",
      "New best score: 0.37524211406707764\n",
      "\n",
      "Epoch 18: train loss: 0.38985956777307323\n",
      "valid loss: 0.37300652265548706\n",
      "New best score: 0.37300652265548706\n",
      "\n",
      "Epoch 19: train loss: 0.3875178318722804\n",
      "valid loss: 0.3708367645740509\n",
      "New best score: 0.3708367645740509\n",
      "\n",
      "Epoch 20: train loss: 0.38642597234338744\n",
      "valid loss: 0.36796143651008606\n",
      "New best score: 0.36796143651008606\n",
      "\n",
      "Epoch 21: train loss: 0.3852645106781694\n",
      "valid loss: 0.36546486616134644\n",
      "New best score: 0.36546486616134644\n",
      "\n",
      "Epoch 22: train loss: 0.3831442119483661\n",
      "valid loss: 0.3637244701385498\n",
      "New best score: 0.3637244701385498\n",
      "\n",
      "Epoch 23: train loss: 0.3814669263990302\n",
      "valid loss: 0.36191946268081665\n",
      "New best score: 0.36191946268081665\n",
      "\n",
      "Epoch 24: train loss: 0.38046525232773976\n",
      "valid loss: 0.3605743646621704\n",
      "New best score: 0.3605743646621704\n",
      "\n",
      "Epoch 25: train loss: 0.3788095127819176\n",
      "valid loss: 0.35900670289993286\n",
      "New best score: 0.35900670289993286\n",
      "\n",
      "Epoch 26: train loss: 0.37777758661965677\n",
      "valid loss: 0.3569689095020294\n",
      "New best score: 0.3569689095020294\n",
      "\n",
      "Epoch 27: train loss: 0.37640323159389927\n",
      "valid loss: 0.3558941185474396\n",
      "New best score: 0.3558941185474396\n",
      "\n",
      "Epoch 28: train loss: 0.37475115907819645\n",
      "valid loss: 0.3543030023574829\n",
      "New best score: 0.3543030023574829\n",
      "\n",
      "Epoch 29: train loss: 0.3738904458687718\n",
      "valid loss: 0.3537033796310425\n",
      "New best score: 0.3537033796310425\n",
      "\n",
      "Epoch 30: train loss: 0.3724735705476058\n",
      "valid loss: 0.35201263427734375\n",
      "New best score: 0.35201263427734375\n",
      "\n",
      "Epoch 31: train loss: 0.3720912726301896\n",
      "valid loss: 0.35102546215057373\n",
      "New best score: 0.35102546215057373\n",
      "\n",
      "Epoch 32: train loss: 0.3710854898718067\n",
      "valid loss: 0.3500666916370392\n",
      "New best score: 0.3500666916370392\n",
      "\n",
      "Epoch 33: train loss: 0.3699209436438137\n",
      "valid loss: 0.34961286187171936\n",
      "New best score: 0.34961286187171936\n",
      "\n",
      "Epoch 34: train loss: 0.3694265893975595\n",
      "valid loss: 0.34737586975097656\n",
      "New best score: 0.34737586975097656\n",
      "\n",
      "Epoch 35: train loss: 0.368140680404534\n",
      "valid loss: 0.3468014597892761\n",
      "New best score: 0.3468014597892761\n",
      "\n",
      "Epoch 36: train loss: 0.3666272309041561\n",
      "valid loss: 0.3459150195121765\n",
      "New best score: 0.3459150195121765\n",
      "\n",
      "Epoch 37: train loss: 0.36567964706205786\n",
      "valid loss: 0.34428924322128296\n",
      "New best score: 0.34428924322128296\n",
      "\n",
      "Epoch 38: train loss: 0.36625765884729256\n",
      "valid loss: 0.34361937642097473\n",
      "New best score: 0.34361937642097473\n",
      "\n",
      "Epoch 39: train loss: 0.36530695311108924\n",
      "valid loss: 0.3418389558792114\n",
      "New best score: 0.3418389558792114\n",
      "\n",
      "Epoch 40: train loss: 0.36491543471365046\n",
      "valid loss: 0.34102171659469604\n",
      "New best score: 0.34102171659469604\n",
      "\n",
      "Epoch 41: train loss: 0.36494805019600945\n",
      "valid loss: 0.3398883640766144\n",
      "New best score: 0.3398883640766144\n",
      "\n",
      "Epoch 42: train loss: 0.3634606487768933\n",
      "valid loss: 0.33908528089523315\n",
      "New best score: 0.33908528089523315\n",
      "\n",
      "Epoch 43: train loss: 0.363055509911444\n",
      "valid loss: 0.33867916464805603\n",
      "New best score: 0.33867916464805603\n",
      "\n",
      "Epoch 44: train loss: 0.36224967572922095\n",
      "valid loss: 0.33687320351600647\n",
      "New best score: 0.33687320351600647\n",
      "\n",
      "Epoch 45: train loss: 0.36036879379946485\n",
      "valid loss: 0.33668261766433716\n",
      "New best score: 0.33668261766433716\n",
      "\n",
      "Epoch 46: train loss: 0.36100107234223444\n",
      "valid loss: 0.33458998799324036\n",
      "New best score: 0.33458998799324036\n",
      "\n",
      "Epoch 47: train loss: 0.36174245969693464\n",
      "valid loss: 0.3348916471004486\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 48: train loss: 0.359431882325868\n",
      "valid loss: 0.3333183526992798\n",
      "New best score: 0.3333183526992798\n",
      "\n",
      "Epoch 49: train loss: 0.35835762140446137\n",
      "valid loss: 0.3322767913341522\n",
      "New best score: 0.3322767913341522\n",
      "\n",
      "Epoch 50: train loss: 0.3602575960464047\n",
      "valid loss: 0.33179232478141785\n",
      "New best score: 0.33179232478141785\n",
      "\n",
      "Epoch 51: train loss: 0.35666451445199493\n",
      "valid loss: 0.3303099274635315\n",
      "New best score: 0.3303099274635315\n",
      "\n",
      "Epoch 52: train loss: 0.3573858899729592\n",
      "valid loss: 0.328643798828125\n",
      "New best score: 0.328643798828125\n",
      "\n",
      "Epoch 53: train loss: 0.35735208333883073\n",
      "valid loss: 0.32811182737350464\n",
      "New best score: 0.32811182737350464\n",
      "\n",
      "Epoch 54: train loss: 0.35600494666207105\n",
      "valid loss: 0.3284516930580139\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 55: train loss: 0.3554096603752079\n",
      "valid loss: 0.32661324739456177\n",
      "New best score: 0.32661324739456177\n",
      "\n",
      "Epoch 56: train loss: 0.3554420627597579\n",
      "valid loss: 0.3255139887332916\n",
      "New best score: 0.3255139887332916\n",
      "\n",
      "Epoch 57: train loss: 0.3543049341305754\n",
      "valid loss: 0.32468605041503906\n",
      "New best score: 0.32468605041503906\n",
      "\n",
      "Epoch 58: train loss: 0.3545121219821442\n",
      "valid loss: 0.3238069415092468\n",
      "New best score: 0.3238069415092468\n",
      "\n",
      "Epoch 59: train loss: 0.3536199386406662\n",
      "valid loss: 0.3238796293735504\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 60: train loss: 0.35370500190813736\n",
      "valid loss: 0.3233132064342499\n",
      "New best score: 0.3233132064342499\n",
      "\n",
      "Epoch 61: train loss: 0.35557476205933364\n",
      "valid loss: 0.32297995686531067\n",
      "New best score: 0.32297995686531067\n",
      "\n",
      "Epoch 62: train loss: 0.3534086114481876\n",
      "valid loss: 0.3218671679496765\n",
      "New best score: 0.3218671679496765\n",
      "\n",
      "Epoch 63: train loss: 0.3540066807341755\n",
      "valid loss: 0.3211405873298645\n",
      "New best score: 0.3211405873298645\n",
      "\n",
      "Epoch 64: train loss: 0.35502437716139884\n",
      "valid loss: 0.32112085819244385\n",
      "New best score: 0.32112085819244385\n",
      "\n",
      "Epoch 65: train loss: 0.3507080068265585\n",
      "valid loss: 0.32036450505256653\n",
      "New best score: 0.32036450505256653\n",
      "\n",
      "Epoch 66: train loss: 0.35034050681537254\n",
      "valid loss: 0.32021671533584595\n",
      "New best score: 0.32021671533584595\n",
      "\n",
      "Epoch 67: train loss: 0.35105229002192506\n",
      "valid loss: 0.31966862082481384\n",
      "New best score: 0.31966862082481384\n",
      "\n",
      "Epoch 68: train loss: 0.35090518867162834\n",
      "valid loss: 0.3189883828163147\n",
      "New best score: 0.3189883828163147\n",
      "\n",
      "Epoch 69: train loss: 0.3509347777617605\n",
      "valid loss: 0.31851550936698914\n",
      "New best score: 0.31851550936698914\n",
      "\n",
      "Epoch 70: train loss: 0.3483106394011275\n",
      "valid loss: 0.3178624212741852\n",
      "New best score: 0.3178624212741852\n",
      "\n",
      "Epoch 71: train loss: 0.34968298176177465\n",
      "valid loss: 0.31804129481315613\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 72: train loss: 0.3507577941829997\n",
      "valid loss: 0.3171325922012329\n",
      "New best score: 0.3171325922012329\n",
      "\n",
      "Epoch 73: train loss: 0.3482304020483691\n",
      "valid loss: 0.31660279631614685\n",
      "New best score: 0.31660279631614685\n",
      "\n",
      "Epoch 74: train loss: 0.3480191323093902\n",
      "valid loss: 0.3165660798549652\n",
      "New best score: 0.3165660798549652\n",
      "\n",
      "Epoch 75: train loss: 0.3470775248412799\n",
      "valid loss: 0.3149036765098572\n",
      "New best score: 0.3149036765098572\n",
      "\n",
      "Epoch 76: train loss: 0.347847523859569\n",
      "valid loss: 0.3150162398815155\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 77: train loss: 0.3484176320689065\n",
      "valid loss: 0.31419336795806885\n",
      "New best score: 0.31419336795806885\n",
      "\n",
      "Epoch 78: train loss: 0.3470961083595018\n",
      "valid loss: 0.3140481412410736\n",
      "New best score: 0.3140481412410736\n",
      "\n",
      "Epoch 79: train loss: 0.34718358897625057\n",
      "valid loss: 0.3138519823551178\n",
      "New best score: 0.3138519823551178\n",
      "\n",
      "Epoch 80: train loss: 0.3463884068610973\n",
      "valid loss: 0.312529593706131\n",
      "New best score: 0.312529593706131\n",
      "\n",
      "Epoch 81: train loss: 0.3488972450109353\n",
      "valid loss: 0.3126509189605713\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 82: train loss: 0.3457947253732753\n",
      "valid loss: 0.31240344047546387\n",
      "New best score: 0.31240344047546387\n",
      "\n",
      "Epoch 83: train loss: 0.3461159032090266\n",
      "valid loss: 0.3124578297138214\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 84: train loss: 0.3450100294629434\n",
      "valid loss: 0.31192946434020996\n",
      "New best score: 0.31192946434020996\n",
      "\n",
      "Epoch 85: train loss: 0.3441489669165217\n",
      "valid loss: 0.3109546899795532\n",
      "New best score: 0.3109546899795532\n",
      "\n",
      "Epoch 86: train loss: 0.3444590503111818\n",
      "valid loss: 0.3106643855571747\n",
      "New best score: 0.3106643855571747\n",
      "\n",
      "Epoch 87: train loss: 0.34317802863013475\n",
      "valid loss: 0.31024134159088135\n",
      "New best score: 0.31024134159088135\n",
      "\n",
      "Epoch 88: train loss: 0.3449491626337955\n",
      "valid loss: 0.3094327449798584\n",
      "New best score: 0.3094327449798584\n",
      "\n",
      "Epoch 89: train loss: 0.3433223343433294\n",
      "valid loss: 0.3099295496940613\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 90: train loss: 0.34340720225993854\n",
      "valid loss: 0.3089454472064972\n",
      "New best score: 0.3089454472064972\n",
      "\n",
      "Epoch 91: train loss: 0.34211849314825876\n",
      "valid loss: 0.30975422263145447\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 92: train loss: 0.34205034119742256\n",
      "valid loss: 0.30891379714012146\n",
      "New best score: 0.30891379714012146\n",
      "\n",
      "Epoch 93: train loss: 0.34386388856665534\n",
      "valid loss: 0.30786842107772827\n",
      "New best score: 0.30786842107772827\n",
      "\n",
      "Epoch 94: train loss: 0.3418879298339213\n",
      "valid loss: 0.3073521852493286\n",
      "New best score: 0.3073521852493286\n",
      "\n",
      "Epoch 95: train loss: 0.34079904143971607\n",
      "valid loss: 0.3077314496040344\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 96: train loss: 0.34242595160814154\n",
      "valid loss: 0.30665966868400574\n",
      "New best score: 0.30665966868400574\n",
      "\n",
      "Epoch 97: train loss: 0.3413947117956061\n",
      "valid loss: 0.30677950382232666\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 98: train loss: 0.34132410713604516\n",
      "valid loss: 0.3080706000328064\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 99: train loss: 0.34153774846765333\n",
      "valid loss: 0.30717411637306213\n",
      "No improvement. Patience 3/5\n"
     ]
    }
   ],
   "source": [
    "# Train with early stopping on Val (normals)\n",
    "model, train_time, _, _ = training_loop(model, train_loader, val_loader, optimizer, criterion,\n",
    "                                        max_epochs=100, apply_early_stopping=True, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57528c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev 70th percentile error threshold = 0.34562326371669766\n"
     ]
    }
   ],
   "source": [
    "# Define 70% threshold based on dev set\n",
    "dev_errs = per_sample_mse(model, dev_loader)\n",
    "thresh = float(np.percentile(dev_errs, 70.0))\n",
    "print(\"Dev 70th percentile error threshold =\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b47c19ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC: 0.8041\n"
     ]
    }
   ],
   "source": [
    "# final test evaluation (UID level) using dev-based threshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Compute errors on test set\n",
    "test_errs = per_sample_mse(model, test_loader)\n",
    "\n",
    "# Construct dictionary to store UID-level errors\n",
    "uid_errors = defaultdict(list)\n",
    "for uid, err in zip(test_df['UID'].values, test_errs):\n",
    "    uid_errors[uid].append(err)\n",
    "\n",
    "# Average error per UID for average rule\n",
    "uid_avg = {u: float(np.mean(es)) for u, es in uid_errors.items()}\n",
    "\n",
    "# Create data frame to store average error per UID\n",
    "uid_df  = pd.DataFrame.from_dict(uid_avg, orient='index', \n",
    "                                  columns=['avg_error'])\n",
    "\n",
    "# Define ground-truth label\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# metrics\n",
    "auc = roc_auc_score(uid_df['true_label'], uid_df['avg_error'])\n",
    "print(f\"UID‐level ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb69c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 70th percentile val-error threshold = 0.7164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "uid_df['pred'] = (uid_df['avg_error'] >= thresh).astype(int)\n",
    "recall   = recall_score(uid_df['true_label'], uid_df['pred'])    # binary‐class recall\n",
    "print(f\"Recall at 70th percentile val-error threshold = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7c670b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC (fraction rule):       0.7385\n",
      "UID‐level recall (majority‐vote rule): 0.7203\n"
     ]
    }
   ],
   "source": [
    "# UID‐level evaluation via fraction‐rule\n",
    "\n",
    "# For each UID, compute fraction of its txns flagged as fraud\n",
    "uid_frac = {u: float((np.array(es) >= thresh).mean()) for u, es in uid_errors.items()}\n",
    "\n",
    "# Build DataFrame and true labels\n",
    "uid_df = pd.DataFrame.from_dict(uid_frac, orient='index',\n",
    "                                columns=['fraud_frac'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# Classify UID as fraud if majority of its txns exceed threshold\n",
    "uid_df['pred'] = (uid_df['fraud_frac'] >= 0.50).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc_frac = roc_auc_score(uid_df['true_label'], uid_df['fraud_frac'])\n",
    "recall_frac = recall_score(uid_df['true_label'], uid_df['pred'])\n",
    "\n",
    "print(f\"UID‐level ROC AUC (fraction rule):       {auc_frac:.4f}\")\n",
    "print(f\"UID‐level recall (majority‐vote rule): {recall_frac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
