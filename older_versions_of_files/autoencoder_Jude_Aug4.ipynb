{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd33dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_transaction.csv\")\n",
    "\n",
    "# derive “day” from TransactionDT, then drop the raw column\n",
    "df[\"day\"] = (df[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "df.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# drop TransactionID, as it is not useful for modeling\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a435cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b01e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card1, card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ProductCD, Unique values: 5\n",
      "Column: card4, Unique values: 4\n",
      "Column: card6, Unique values: 4\n",
      "Column: P_emaildomain, Unique values: 59\n",
      "Column: R_emaildomain, Unique values: 60\n",
      "Column: M1, Unique values: 2\n",
      "Column: M2, Unique values: 2\n",
      "Column: M3, Unique values: 2\n",
      "Column: M4, Unique values: 3\n",
      "Column: M5, Unique values: 2\n",
      "Column: M6, Unique values: 2\n",
      "Column: M7, Unique values: 2\n",
      "Column: M8, Unique values: 2\n",
      "Column: M9, Unique values: 2\n",
      "Column: card1, Unique values: 13553\n",
      "Column: card2, Unique values: 500\n",
      "Column: card3, Unique values: 114\n",
      "Column: card5, Unique values: 119\n",
      "Column: addr1, Unique values: 332\n",
      "Column: addr2, Unique values: 74\n"
     ]
    }
   ],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones else drop them\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# extract high cardinality categorical columns\n",
    "high_card_cols = [c for c in cat_cols_rem if df[c].nunique() > 10]\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12be534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical columns: ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\n"
     ]
    }
   ],
   "source": [
    "print(f\"High cardinality categorical columns: {high_card_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>M1_T</th>\n",
       "      <th>M2_T</th>\n",
       "      <th>M3_T</th>\n",
       "      <th>M4_M1</th>\n",
       "      <th>M4_M2</th>\n",
       "      <th>M5_T</th>\n",
       "      <th>M6_T</th>\n",
       "      <th>M7_T</th>\n",
       "      <th>M8_T</th>\n",
       "      <th>M9_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 346 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  card1  card2  card3  card5  addr1  addr2  dist1  \\\n",
       "0        0            68.5  13926    NaN  150.0  142.0  315.0   87.0   19.0   \n",
       "1        0            29.0   2755  404.0  150.0  102.0  325.0   87.0    8.0   \n",
       "2        0            59.0   4663  490.0  150.0  166.0  330.0   87.0  287.0   \n",
       "3        0            50.0  18132  567.0  150.0  117.0  476.0   87.0    8.0   \n",
       "4        0            50.0   4497  514.0  150.0  102.0  420.0   87.0    8.0   \n",
       "\n",
       "  P_emaildomain  ... M1_T  M2_T  M3_T  M4_M1  M4_M2  M5_T  M6_T  M7_T  M8_T  \\\n",
       "0           NaN  ...    1     1     1      0      1     0     1     0     0   \n",
       "1     gmail.com  ...    0     0     0      0      0     1     1     0     0   \n",
       "2   outlook.com  ...    1     1     1      0      0     0     0     0     0   \n",
       "3     yahoo.com  ...    0     0     0      0      0     1     0     0     0   \n",
       "4     gmail.com  ...    0     0     0      0      0     0     0     0     0   \n",
       "\n",
       "   M9_T  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 346 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode high cardinality categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Store encoders for later inference\n",
    "encoders = {}\n",
    "for col in high_card_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b823102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all columns from high cardinality categorical columns\n",
    "other_cols = [c for c in df.columns if c not in high_card_cols]\n",
    "other_cols.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d4311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Training on normals only: (455901, 345)\n",
      "→ CV set (normals+fraud): (134639, 345)\n"
     ]
    }
   ],
   "source": [
    "# Create training and CV sets\n",
    "# all non‐fraud examples\n",
    "df_norm = df[df.isFraud == 0].copy()\n",
    "# all fraud examples\n",
    "df_fraud = df[df.isFraud == 1].copy()\n",
    "\n",
    "# hold out 20% of normals for CV\n",
    "norm_train, norm_cv = train_test_split(\n",
    "    df_norm, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# CV set = held‐out normals + all frauds\n",
    "df_cv = pd.concat([norm_cv, df_fraud], axis=0)\n",
    "y_cv  = df_cv[\"isFraud\"].values\n",
    "\n",
    "# drop labels for modeling\n",
    "X_train = norm_train.drop(\"isFraud\", axis=1)\n",
    "X_cv    = df_cv.drop(\"isFraud\", axis=1)\n",
    "\n",
    "print(\"→ Training on normals only:\", X_train.shape)\n",
    "print(\"→ CV set (normals+fraud):\", X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "non_one_hot_cols = [col for col in X_train.columns if col not in one_hot_cols and col not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_cv_scaled    = X_cv.copy()\n",
    "\n",
    "X_train_scaled[non_one_hot_cols] = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_cv_scaled[non_one_hot_cols]    = scaler.transform(X_cv[non_one_hot_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e6f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_num_train = torch.tensor(X_train_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_train = torch.tensor(X_train_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_val = torch.tensor(X_cv_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_val = torch.tensor(X_cv_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "y_valid = torch.FloatTensor(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "911478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_num_train, X_cat_train, X_num_train, X_cat_train)\n",
    "val_ds   = TensorDataset(X_num_val, X_cat_val, X_num_val, X_cat_val)\n",
    "\n",
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73448abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2):\n",
    "        super(AutoEncoderWithEmbeddings, self).__init__()\n",
    "        \n",
    "        # Create embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, min(50, (num_categories + 1)//2))\n",
    "            for num_categories in cat_cardinalities\n",
    "        ])\n",
    "        \n",
    "        emb_size_total = sum([emb.embedding_dim for emb in self.embeddings])\n",
    "        total_input_size = num_numeric + emb_size_total\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(total_input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, hidden2)\n",
    "        self.fc5 = nn.Linear(hidden2, hidden1)\n",
    "        self.fc6 = nn.Linear(hidden1, total_input_size)\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Embed categorical variables\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embeds = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        # Concatenate numerical + embeddings\n",
    "        x = torch.cat([x_num, embeds], dim=1)\n",
    "\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a15fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fe09b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9981a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numeric features (including one-hot encodings)\n",
    "num_numeric = len(other_cols)\n",
    "\n",
    "# Cardinalities (unique values) of each categorical feature\n",
    "cat_cardinalities = [df[col].nunique() for col in high_card_cols]\n",
    "\n",
    "# Initialize the embedding autoencoder model\n",
    "seed_everything(SEED)\n",
    "model = AutoEncoderWithEmbeddings(num_numeric=num_numeric, \n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            hidden1=128, hidden2=64, code_size=8, \n",
    "                            dropout_rate=0.2)\n",
    "\n",
    "losses = per_sample_mse(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float32(0.56106347), np.float32(2.0109246), np.float32(0.55420184), np.float32(0.6527646), np.float32(0.9973502)]\n",
      "1.4967284\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66da7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,generator,criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "        # Construct full target (numeric + embeddings)\n",
    "        true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "        y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred, y_true_full)\n",
    "        batch_losses.append(loss.item())\n",
    "    mean_loss = np.mean(batch_losses)    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04b30bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "    \n",
    "    def continue_training(self,current_score):\n",
    "        if self.best_score > current_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(\"New best score:\", current_score)\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(self.counter, \" iterations since best score.\")\n",
    "                \n",
    "        return self.counter <= self.patience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4465904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    #Setting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    \n",
    "    #Training loop\n",
    "    start_time=time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = [model.embeddings[i](yb_cat[:, i]) for i in range(len(model.embeddings))]\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, y_true_full)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        #showing last training loss after each epoch\n",
    "        all_train_losses.append(np.mean(train_loss))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        #evaluating the model on the test set after each epoch    \n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.continue_training(valid_loss):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "    training_execution_time=time.time()-start_time\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d616349",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef8007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.7100744528735661\n",
      "valid loss: 1.0759438130112202\n",
      "New best score: 1.0759438130112202\n",
      "\n",
      "Epoch 1: train loss: 0.5721140254694471\n",
      "valid loss: 0.9307100716199258\n",
      "New best score: 0.9307100716199258\n",
      "\n",
      "Epoch 2: train loss: 0.5084841774837203\n",
      "valid loss: 0.8701037218362206\n",
      "New best score: 0.8701037218362206\n",
      "\n",
      "Epoch 3: train loss: 0.46317482911108854\n",
      "valid loss: 0.8159573852109365\n",
      "New best score: 0.8159573852109365\n",
      "\n",
      "Epoch 4: train loss: 0.4268100562721792\n",
      "valid loss: 0.7773280569809018\n",
      "New best score: 0.7773280569809018\n",
      "\n",
      "Epoch 5: train loss: 0.3986800983297303\n",
      "valid loss: 0.7464546328941679\n",
      "New best score: 0.7464546328941679\n",
      "\n",
      "Epoch 6: train loss: 0.37613097525606254\n",
      "valid loss: 0.7225671778840257\n",
      "New best score: 0.7225671778840257\n",
      "\n",
      "Epoch 7: train loss: 0.3561944396608205\n",
      "valid loss: 0.7017435869449898\n",
      "New best score: 0.7017435869449898\n",
      "\n",
      "Epoch 8: train loss: 0.33799978398313424\n",
      "valid loss: 0.6796386339365302\n",
      "New best score: 0.6796386339365302\n",
      "\n",
      "Epoch 9: train loss: 0.32099408852866734\n",
      "valid loss: 0.66036396159192\n",
      "New best score: 0.66036396159192\n",
      "\n",
      "Epoch 10: train loss: 0.30729393048920617\n",
      "valid loss: 0.6423429317574084\n",
      "New best score: 0.6423429317574084\n",
      "\n",
      "Epoch 11: train loss: 0.2941665640048574\n",
      "valid loss: 0.6272789337335883\n",
      "New best score: 0.6272789337335883\n",
      "\n",
      "Epoch 12: train loss: 0.28278873230337964\n",
      "valid loss: 0.6081350804395096\n",
      "New best score: 0.6081350804395096\n",
      "\n",
      "Epoch 13: train loss: 0.27172982304719695\n",
      "valid loss: 0.5873291066390933\n",
      "New best score: 0.5873291066390933\n",
      "\n",
      "Epoch 14: train loss: 0.2619064720525217\n",
      "valid loss: 0.5658283661413556\n",
      "New best score: 0.5658283661413556\n",
      "\n",
      "Epoch 15: train loss: 0.252062595686661\n",
      "valid loss: 0.5520482164712006\n",
      "New best score: 0.5520482164712006\n",
      "\n",
      "Epoch 16: train loss: 0.2445799812390213\n",
      "valid loss: 0.5317009334441827\n",
      "New best score: 0.5317009334441827\n",
      "\n",
      "Epoch 17: train loss: 0.2367914701852734\n",
      "valid loss: 0.5177108367246367\n",
      "New best score: 0.5177108367246367\n",
      "\n",
      "Epoch 18: train loss: 0.23083119350250322\n",
      "valid loss: 0.5006494628045949\n",
      "New best score: 0.5006494628045949\n",
      "\n",
      "Epoch 19: train loss: 0.2254187522412149\n",
      "valid loss: 0.491632605205244\n",
      "New best score: 0.491632605205244\n",
      "\n",
      "Epoch 20: train loss: 0.2195529747256794\n",
      "valid loss: 0.4797203596043949\n",
      "New best score: 0.4797203596043949\n",
      "\n",
      "Epoch 21: train loss: 0.21507072754781253\n",
      "valid loss: 0.4719273775759305\n",
      "New best score: 0.4719273775759305\n",
      "\n",
      "Epoch 22: train loss: 0.20986779232318153\n",
      "valid loss: 0.46620347931584477\n",
      "New best score: 0.46620347931584477\n",
      "\n",
      "Epoch 23: train loss: 0.20620680790223123\n",
      "valid loss: 0.4556202594431634\n",
      "New best score: 0.4556202594431634\n",
      "\n",
      "Epoch 24: train loss: 0.20202828196162728\n",
      "valid loss: 0.45272925643413237\n",
      "New best score: 0.45272925643413237\n",
      "\n",
      "Epoch 25: train loss: 0.1981399198607296\n",
      "valid loss: 0.44404553131912144\n",
      "New best score: 0.44404553131912144\n",
      "\n",
      "Epoch 26: train loss: 0.1959552946181677\n",
      "valid loss: 0.4395316251103869\n",
      "New best score: 0.4395316251103869\n",
      "\n",
      "Epoch 27: train loss: 0.192346384362072\n",
      "valid loss: 0.43631970975902146\n",
      "New best score: 0.43631970975902146\n",
      "\n",
      "Epoch 28: train loss: 0.19207675345283834\n",
      "valid loss: 0.433382869786636\n",
      "New best score: 0.433382869786636\n",
      "\n",
      "Epoch 29: train loss: 0.18944387210639638\n",
      "valid loss: 0.43278873556013797\n",
      "New best score: 0.43278873556013797\n",
      "\n",
      "Epoch 30: train loss: 0.18717831756196307\n",
      "valid loss: 0.4279945172057859\n",
      "New best score: 0.4279945172057859\n",
      "\n",
      "Epoch 31: train loss: 0.1864289627666559\n",
      "valid loss: 0.4265530514796424\n",
      "New best score: 0.4265530514796424\n",
      "\n",
      "Epoch 32: train loss: 0.18500606386214663\n",
      "valid loss: 0.4249135171392571\n",
      "New best score: 0.4249135171392571\n",
      "\n",
      "Epoch 33: train loss: 0.18184386598475186\n",
      "valid loss: 0.4205687119140371\n",
      "New best score: 0.4205687119140371\n",
      "\n",
      "Epoch 34: train loss: 0.18027926896992488\n",
      "valid loss: 0.41615359292510795\n",
      "New best score: 0.41615359292510795\n",
      "\n",
      "Epoch 35: train loss: 0.17984114429264358\n",
      "valid loss: 0.41667460154444547\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 36: train loss: 0.17911896061081142\n",
      "valid loss: 0.409701993171933\n",
      "New best score: 0.409701993171933\n",
      "\n",
      "Epoch 37: train loss: 0.17741505080033365\n",
      "valid loss: 0.4041410948897043\n",
      "New best score: 0.4041410948897043\n",
      "\n",
      "Epoch 38: train loss: 0.1770127678141583\n",
      "valid loss: 0.4031479792109914\n",
      "New best score: 0.4031479792109914\n",
      "\n",
      "Epoch 39: train loss: 0.17621364777303572\n",
      "valid loss: 0.39874526487217204\n",
      "New best score: 0.39874526487217204\n",
      "\n",
      "Epoch 40: train loss: 0.17517801986298578\n",
      "valid loss: 0.3935689959859213\n",
      "New best score: 0.3935689959859213\n",
      "\n",
      "Epoch 41: train loss: 0.17312814788652456\n",
      "valid loss: 0.3951309841451989\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 42: train loss: 0.1725531107130409\n",
      "valid loss: 0.3894588203654543\n",
      "New best score: 0.3894588203654543\n",
      "\n",
      "Epoch 43: train loss: 0.17073570645082947\n",
      "valid loss: 0.39040405864158056\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 44: train loss: 0.16996249032241326\n",
      "valid loss: 0.3858681533896424\n",
      "New best score: 0.3858681533896424\n",
      "\n",
      "Epoch 45: train loss: 0.1684580019494366\n",
      "valid loss: 0.38605032475955586\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 46: train loss: 0.16941125858177908\n",
      "valid loss: 0.3821719335285883\n",
      "New best score: 0.3821719335285883\n",
      "\n",
      "Epoch 47: train loss: 0.16954711644035397\n",
      "valid loss: 0.380231755031379\n",
      "New best score: 0.380231755031379\n",
      "\n",
      "Epoch 48: train loss: 0.16746262721006583\n",
      "valid loss: 0.3789132881073897\n",
      "New best score: 0.3789132881073897\n",
      "\n",
      "Epoch 49: train loss: 0.16682386186755735\n",
      "valid loss: 0.37747211850641343\n",
      "New best score: 0.37747211850641343\n",
      "\n",
      "Epoch 50: train loss: 0.16613725510109154\n",
      "valid loss: 0.3758331154235869\n",
      "New best score: 0.3758331154235869\n",
      "\n",
      "Epoch 51: train loss: 0.16637769664243682\n",
      "valid loss: 0.37184011191129684\n",
      "New best score: 0.37184011191129684\n",
      "\n",
      "Epoch 52: train loss: 0.1645221660785402\n",
      "valid loss: 0.37311362860433955\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 53: train loss: 0.16459968916896217\n",
      "valid loss: 0.37247220109510787\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 54: train loss: 0.1625767048243454\n",
      "valid loss: 0.36976917195682746\n",
      "New best score: 0.36976917195682746\n",
      "\n",
      "Epoch 55: train loss: 0.16475821078225017\n",
      "valid loss: 0.3740667066759936\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 56: train loss: 0.16148690564095908\n",
      "valid loss: 0.3703850931824387\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 57: train loss: 0.16246019560513406\n",
      "valid loss: 0.37172440530116113\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 58: train loss: 0.1596132231966146\n",
      "valid loss: 0.369928198792182\n",
      "4  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,train_loader,val_loader,optimizer,criterion,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4baa1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float32(0.0038353705), np.float32(0.53931046), np.float32(0.008435252), np.float32(0.019826617), np.float32(0.29362282)]\n",
      "0.36923152\n"
     ]
    }
   ],
   "source": [
    "losses = per_sample_mse(model, val_loader)\n",
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9855e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fraud reconstruction error: 1.7159481\n",
      "Average genuine reconstruction error: 0.12508184\n"
     ]
    }
   ],
   "source": [
    "genuine_losses = np.array(losses)[y_valid.numpy() == 0]\n",
    "fraud_losses = np.array(losses)[y_valid.numpy() == 1]\n",
    "print(\"Average fraud reconstruction error:\", np.mean(fraud_losses))\n",
    "print(\"Average genuine reconstruction error:\", np.mean(genuine_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f88db505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score)\n",
    "\n",
    "# compute AUC-ROC and Average Precision on the validation set by considering the reconstruction errors as predicted fraud scores\n",
    "\n",
    "AUC_ROC = roc_auc_score(y_cv, losses)\n",
    "AP = average_precision_score(y_cv, losses)\n",
    "    \n",
    "performances = pd.DataFrame([[AUC_ROC, AP]], columns=['AUC ROC','Average precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd5bf37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.74295</td>\n",
       "      <td>0.475899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AUC ROC  Average precision\n",
       "0  0.74295           0.475899"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3ccea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at threshold 0.1027 = 0.6271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "thr      = np.percentile(losses, 70)      # e.g. top 30% as “fraud”\n",
    "y_pred   = (losses >= thr).astype(int)\n",
    "recall   = recall_score(y_cv, y_pred)    # binary‐class recall\n",
    "print(f\"Recall at threshold {thr:.4f} = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a11696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
