{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Utils\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a2cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Update these paths if running on a different machine.\n",
    "txn = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_transaction.csv\")\n",
    "idm = pd.read_csv(\"/Users/judepereira/Downloads/ieee-fraud-detection/train_identity.csv\")\n",
    "\n",
    "# Derive simple features (day, D1new) and drop TransactionDT\n",
    "txn[\"day\"] = (txn[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "txn[\"D1new\"] = (txn[\"TransactionDT\"] // (60*60*24)) - txn[\"D1\"] + 2000\n",
    "txn.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# Merge identity into transactions, drop TransactionID afterward\n",
    "df = txn.merge(idm, on=\"TransactionID\", how=\"left\")\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657089dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UID construction helpers\n",
    "def safe_str(series):\n",
    "    \"\"\"Convert to string with an explicit missing token to avoid 'nan' ambiguity.\"\"\"\n",
    "    return series.astype(\"object\").where(~series.isna(), \"__MISSING__\").astype(str)\n",
    "\n",
    "def compute_uids(df):\n",
    "    \"\"\"Stable UID built from card/address/email/C1 with explicit missing token.\"\"\"\n",
    "    return (\n",
    "        safe_str(df['card1']) + \"_\" +\n",
    "        safe_str(df['addr1']) + \"_\" +\n",
    "        safe_str(df['D1new']) + \"_\" +\n",
    "        safe_str(df['P_emaildomain']) + \"_\" +\n",
    "        safe_str(df['C1'])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39440759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: id_12, Unique values: 2\n",
      "One-hot encoding id_12 with 2 unique values\n",
      "Column: id_13, Unique values: 54\n",
      "Column: id_15, Unique values: 3\n",
      "One-hot encoding id_15 with 3 unique values\n",
      "Column: id_16, Unique values: 2\n",
      "One-hot encoding id_16 with 2 unique values\n",
      "Column: id_17, Unique values: 104\n",
      "Column: id_19, Unique values: 522\n",
      "Column: id_20, Unique values: 394\n",
      "Column: id_28, Unique values: 2\n",
      "One-hot encoding id_28 with 2 unique values\n",
      "Column: id_29, Unique values: 2\n",
      "One-hot encoding id_29 with 2 unique values\n",
      "Column: id_31, Unique values: 130\n",
      "Column: id_35, Unique values: 2\n",
      "One-hot encoding id_35 with 2 unique values\n",
      "Column: id_36, Unique values: 2\n",
      "One-hot encoding id_36 with 2 unique values\n",
      "Column: id_37, Unique values: 2\n",
      "One-hot encoding id_37 with 2 unique values\n",
      "Column: id_38, Unique values: 2\n",
      "One-hot encoding id_38 with 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# categorical ID columns\n",
    "id_cols = [c for c in df.columns if c.startswith(\"id_\")]\n",
    "\n",
    "# but id_01 to id_11 are numerical so need to exclude them\n",
    "id_cat_cols = [c for c in id_cols if not c.startswith(\"id_0\")]\n",
    "id_cat_cols.remove(\"id_11\")  # id_11 is a numerical column\n",
    "\n",
    "# extract high cardinality categorical ID columns\n",
    "id_high_card_cols = [c for c in id_cat_cols if df[c].nunique() > 10]\n",
    "\n",
    "# one-hot encode categorical features with low cardinality\n",
    "for c in id_cat_cols:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2549cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs to be imputed: ['V128', 'V26', 'V232', 'V69', 'V182', 'V106', 'V320', 'V122', 'V257', 'dist1', 'V10', 'V307', 'V99', 'V22', 'V290', 'V171', 'V264', 'V212', 'V88', 'V243', 'V61', 'V296', 'V245', 'V60', 'V20', 'V115', 'V293', 'V67', 'V105', 'V62', 'V169', 'V36', 'id_11', 'V283', 'V35', 'id_17', 'V47', 'V227', 'V63', 'V310', 'V284', 'V17', 'V9', 'V319', 'V111', 'V39', 'V66', 'V291', 'V268', 'V83', 'V311', 'V321', 'D4', 'D11', 'V101', 'V215', 'V118', 'V276', 'V42', 'V224', 'V206', 'V112', 'V104', 'V219', 'V306', 'V124', 'V82', 'V134', 'V24', 'V110', 'V91', 'V222', 'V57', 'V248', 'V187', 'V192', 'V258', 'V175', 'V230', 'V198', 'V54', 'V41', 'V266', 'V262', 'V120', 'V263', 'V279', 'V304', 'V51', 'V71', 'V170', 'V226', 'V107', 'V177', 'V267', 'V173', 'V221', 'V302', 'V38', 'V183', 'V75', 'V53', 'V2', 'V195', 'V31', 'D3', 'V135', 'V259', 'V318', 'V96', 'V172', 'V70', 'V43', 'V236', 'D5', 'V207', 'V241', 'V210', 'V186', 'V220', 'V19', 'V133', 'V208', 'V269', 'V242', 'V308', 'V314', 'V11', 'V90', 'V94', 'V127', 'V168', 'V15', 'V102', 'V189', 'V197', 'D15', 'V298', 'V64', 'V239', 'V32', 'V50', 'V250', 'id_02', 'V181', 'V205', 'V238', 'D1new', 'V114', 'V45', 'V5', 'V256', 'V121', 'V84', 'V216', 'V49', 'V100', 'V79', 'V297', 'V7', 'V119', 'V52', 'V240', 'V37', 'V77', 'V260', 'V167', 'V126', 'V95', 'V4', 'V285', 'V191', 'V14', 'V30', 'D2', 'V301', 'V176', 'V34', 'V312', 'id_06', 'V65', 'V315', 'V72', 'V46', 'V68', 'V199', 'V231', 'V278', 'V292', 'V108', 'V80', 'V237', 'V274', 'V25', 'V3', 'V93', 'V184', 'V234', 'V252', 'V288', 'V33', 'V131', 'V270', 'V48', 'V303', 'D10', 'V132', 'V294', 'V313', 'V23', 'V73', 'V272', 'V211', 'V28', 'V214', 'V233', 'V271', 'V8', 'V309', 'V317', 'V55', 'V300', 'V223', 'V97', 'V117', 'id_19', 'V188', 'V81', 'V16', 'V281', 'V123', 'V203', 'V229', 'V235', 'V27', 'V275', 'V74', 'V58', 'V89', 'V225', 'V1', 'V200', 'V178', 'V289', 'V193', 'V247', 'V129', 'V180', 'V40', 'V246', 'V202', 'V254', 'V218', 'V282', 'id_13', 'V179', 'V87', 'V185', 'V113', 'V251', 'V6', 'V76', 'V12', 'V316', 'V194', 'V280', 'V98', 'V261', 'V21', 'V174', 'V56', 'V201', 'V13', 'V44', 'V273', 'id_01', 'V103', 'id_20', 'V299', 'V196', 'V265', 'V59', 'V286', 'V109', 'V92', 'V130', 'V125', 'V217', 'V295', 'V78', 'V137', 'V86', 'V228', 'D1', 'V213', 'V18', 'V85', 'id_05', 'V249', 'V253', 'V277', 'V116', 'V190', 'V136', 'V204', 'V305', 'V209', 'V244', 'V287', 'V255', 'V29']\n"
     ]
    }
   ],
   "source": [
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# include id_01 to id_11 as they are numerical\n",
    "id_num_cols = [c for c in id_cols if c not in id_cat_cols]\n",
    "num_cols.extend(id_num_cols)\n",
    "\n",
    "# remove duplicates from the list\n",
    "num_cols = list(set(num_cols))\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card1, card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card1\", \"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "print(f\"Columns with NaNs to be imputed: {nan_cols}\")\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26124b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: M3, Unique values: 2\n",
      "One-hot encoding M3 with 2 unique values\n",
      "Column: M5, Unique values: 2\n",
      "One-hot encoding M5 with 2 unique values\n",
      "Column: M6, Unique values: 2\n",
      "One-hot encoding M6 with 2 unique values\n",
      "Column: card1, Unique values: 13553\n",
      "Column: M4, Unique values: 3\n",
      "One-hot encoding M4 with 3 unique values\n",
      "Column: addr1, Unique values: 332\n",
      "Column: R_emaildomain, Unique values: 60\n",
      "Column: DeviceType, Unique values: 2\n",
      "One-hot encoding DeviceType with 2 unique values\n",
      "Column: M2, Unique values: 2\n",
      "One-hot encoding M2 with 2 unique values\n",
      "Column: M7, Unique values: 2\n",
      "One-hot encoding M7 with 2 unique values\n",
      "Column: card5, Unique values: 119\n",
      "Column: addr2, Unique values: 74\n",
      "Column: card3, Unique values: 114\n",
      "Column: card2, Unique values: 500\n",
      "Column: card4, Unique values: 4\n",
      "One-hot encoding card4 with 4 unique values\n",
      "Column: card6, Unique values: 4\n",
      "One-hot encoding card6 with 4 unique values\n",
      "Column: M1, Unique values: 2\n",
      "One-hot encoding M1 with 2 unique values\n",
      "Column: P_emaildomain, Unique values: 59\n",
      "Column: ProductCD, Unique values: 5\n",
      "One-hot encoding ProductCD with 5 unique values\n",
      "Column: M8, Unique values: 2\n",
      "One-hot encoding M8 with 2 unique values\n",
      "Column: DeviceInfo, Unique values: 1786\n",
      "Column: id_31, Unique values: 130\n",
      "Column: M9, Unique values: 2\n",
      "One-hot encoding M9 with 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# remove duplicates from the list\n",
    "cat_cols_rem = list(set(cat_cols_rem))\n",
    "\n",
    "# extract high cardinality categorical columns\n",
    "high_card_cols = [c for c in cat_cols_rem if df[c].nunique() > 10]\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    print(f\"Column: {c}, Unique values: {n_uniq}\")\n",
    "    if n_uniq <= 10:\n",
    "        print(f\"One-hot encoding {c} with {n_uniq} unique values\")\n",
    "        dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "        df = pd.concat([df.drop(c, axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12be534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High cardinality categorical columns: ['id_31', 'card5', 'card1', 'addr2', 'card3', 'id_13', 'addr1', 'R_emaildomain', 'id_17', 'card2', 'id_19', 'P_emaildomain', 'DeviceInfo', 'id_20']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# include the high cardinality categorical ID columns but remove duplicates\n",
    "high_card_cols = list(set(high_card_cols + id_high_card_cols))  # remove duplicates\n",
    "print(f\"High cardinality categorical columns: {high_card_cols}\")\n",
    "print(len(high_card_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40bb5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38d6a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>card6_credit</th>\n",
       "      <th>card6_debit</th>\n",
       "      <th>card6_debit or credit</th>\n",
       "      <th>M1_T</th>\n",
       "      <th>ProductCD_H</th>\n",
       "      <th>ProductCD_R</th>\n",
       "      <th>ProductCD_S</th>\n",
       "      <th>ProductCD_W</th>\n",
       "      <th>M8_T</th>\n",
       "      <th>M9_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>outlook.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   isFraud  TransactionAmt  card1  card2  card3  card5  addr1  addr2  dist1  \\\n",
       "0        0            68.5  13926    NaN  150.0  142.0  315.0   87.0   19.0   \n",
       "1        0            29.0   2755  404.0  150.0  102.0  325.0   87.0    8.0   \n",
       "2        0            59.0   4663  490.0  150.0  166.0  330.0   87.0  287.0   \n",
       "3        0            50.0  18132  567.0  150.0  117.0  476.0   87.0    8.0   \n",
       "4        0            50.0   4497  514.0  150.0  102.0  420.0   87.0    8.0   \n",
       "\n",
       "  P_emaildomain  ... card6_credit  card6_debit  card6_debit or credit  M1_T  \\\n",
       "0           NaN  ...            1            0                      0     1   \n",
       "1     gmail.com  ...            1            0                      0     0   \n",
       "2   outlook.com  ...            0            1                      0     1   \n",
       "3     yahoo.com  ...            0            1                      0     0   \n",
       "4     gmail.com  ...            1            0                      0     0   \n",
       "\n",
       "   ProductCD_H  ProductCD_R  ProductCD_S  ProductCD_W  M8_T  M9_T  \n",
       "0            0            0            0            1     0     0  \n",
       "1            0            0            0            1     0     0  \n",
       "2            0            0            0            1     0     0  \n",
       "3            0            0            0            1     0     0  \n",
       "4            1            0            0            0     0     0  \n",
       "\n",
       "[5 rows x 369 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2c1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode high cardinality categorical columns and\n",
    "# Store encoders for later inference\n",
    "encoders = {}\n",
    "for col in high_card_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b823102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate all columns from high cardinality categorical columns\n",
    "other_cols = [c for c in df.columns if c not in high_card_cols]\n",
    "other_cols.remove(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0021ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train(normals) UIDs: 187885  | rows: 340202\n",
      "Val(normals)   UIDs: 62628    | rows: 113875\n",
      "Test(mix)      UIDs: 74792   | rows: 136463\n"
     ]
    }
   ],
   "source": [
    "# Faster way to split UIDs into train/val/test sets\n",
    "# Create UID column using the robust helper\n",
    "df['UID'] = compute_uids(df)\n",
    "\n",
    "# Label UIDs as fraud if any txn is fraud, then split\n",
    "uid_labels = df.groupby('UID')['isFraud'].max().rename('UID_isFraud')\n",
    "df = df.merge(uid_labels, left_on='UID', right_index=True)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "normal_uids = uid_labels[uid_labels == 0].index.values\n",
    "fraud_uids  = uid_labels[uid_labels == 1].index.values\n",
    "\n",
    "# Split normal UIDs as 60% in train, 20% in val, 20% in hold\n",
    "rng.shuffle(normal_uids)\n",
    "n_norm = len(normal_uids)\n",
    "n_train = int(0.60 * n_norm)\n",
    "n_val   = int(0.20 * n_norm)\n",
    "\n",
    "train_norm_uids = normal_uids[:n_train]\n",
    "val_norm_uids   = normal_uids[n_train:n_train+n_val]\n",
    "hold_norm_uids  = normal_uids[n_train+n_val:]\n",
    "\n",
    "train_uids = set(train_norm_uids)\n",
    "val_uids   = set(val_norm_uids)\n",
    "test_uids  = set(hold_norm_uids) | set(fraud_uids)\n",
    "\n",
    "def slice_by_uids(df_in, uids):\n",
    "    return df_in[df_in['UID'].isin(uids)].copy()\n",
    "\n",
    "train_df = slice_by_uids(df, train_uids)  # normals only\n",
    "val_df   = slice_by_uids(df, val_uids)    # normals only\n",
    "test_df  = slice_by_uids(df, test_uids)   # mixture\n",
    "\n",
    "print(f\"Train(normals) UIDs: {len(train_uids)}  | rows: {train_df.shape[0]}\")\n",
    "print(f\"Val(normals)   UIDs: {len(val_uids)}    | rows: {val_df.shape[0]}\")\n",
    "print(f\"Test(mix)      UIDs: {len(test_uids)}   | rows: {test_df.shape[0]}\")\n",
    "\n",
    "# Feature matrices per split (drop label & UID)\n",
    "X_train = train_df.drop(['isFraud','UID'], axis=1)\n",
    "X_val   = val_df  .drop(['isFraud','UID'], axis=1)\n",
    "X_test  = test_df .drop(['isFraud','UID'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7cbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify one-hot columns (all values are 0 or 1)\n",
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "\n",
    "# Define columns to be standardized\n",
    "std_cols = [col for col in X_train.columns if col not in one_hot_cols and col not in high_card_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a6cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled    = X_val.copy()\n",
    "X_test_scaled   = X_test.copy()\n",
    "\n",
    "X_train_scaled[std_cols] = scaler.fit_transform(X_train[std_cols])\n",
    "X_val_scaled[std_cols]   = scaler.transform(X_val[std_cols])\n",
    "X_test_scaled[std_cols]  = scaler.transform(X_test[std_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e6f77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_num_train = torch.tensor(X_train_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_train = torch.tensor(X_train_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_val = torch.tensor(X_val_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_val = torch.tensor(X_val_scaled[high_card_cols].values, dtype=torch.long)\n",
    "\n",
    "X_num_test = torch.tensor(X_test_scaled[other_cols].values, dtype=torch.float32)\n",
    "X_cat_test = torch.tensor(X_test_scaled[high_card_cols].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911478c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets for train/val/test sets\n",
    "train_ds = TensorDataset(X_num_train, X_cat_train, X_num_train, X_cat_train)\n",
    "val_ds   = TensorDataset(X_num_val, X_cat_val, X_num_val, X_cat_val)\n",
    "test_ds  = TensorDataset(X_num_test, X_cat_test, X_num_test, X_cat_test)\n",
    "\n",
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73448abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition with categorical embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AutoEncoderWithEmbeddings(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_cardinalities, hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2):\n",
    "        super(AutoEncoderWithEmbeddings, self).__init__()\n",
    "        \n",
    "        # Create embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, min(50, (num_categories + 1)//2))\n",
    "            for num_categories in cat_cardinalities\n",
    "        ])\n",
    "        \n",
    "        emb_size_total = sum([emb.embedding_dim for emb in self.embeddings])\n",
    "        total_input_size = num_numeric + emb_size_total\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(total_input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, hidden2)\n",
    "        self.fc5 = nn.Linear(hidden2, hidden1)\n",
    "        self.fc6 = nn.Linear(hidden1, total_input_size)\n",
    "    \n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Embed categorical variables\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embeds = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        # Concatenate numerical + embeddings\n",
    "        x = torch.cat([x_num, embeds], dim=1)\n",
    "\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee4103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def per_sample_mse(model, generator):\n",
    "    # Targets are built from *detached* embedding lookups to avoid target leakage\n",
    "    model.eval()\n",
    "    crit = torch.nn.MSELoss(reduction=\"none\")\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = []\n",
    "            if xb_cat.shape[1] > 0:\n",
    "                for i in range(len(model.embeddings)):\n",
    "                    true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "\n",
    "            # Compute Loss\n",
    "            l = crit(y_pred, y_true_full).mean(dim=1).cpu().numpy()\n",
    "            losses.extend(l.tolist())\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e35e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model,generator,criterion):\n",
    "    # Detached embedding targets for validation loss\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in generator:\n",
    "            # Forward pass\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "\n",
    "            # Construct full target (numeric + embeddings)\n",
    "            true_embeds = []\n",
    "            if xb_cat.shape[1] > 0:\n",
    "                for i in range(len(model.embeddings)):\n",
    "                    true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "            y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred, y_true_full).mean(dim=1)\n",
    "            batch_losses.extend(list(loss.cpu().numpy()))\n",
    "    return float(np.mean(batch_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "649b1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,\n",
    "                  max_epochs=100,apply_early_stopping=True,patience=3,verbose=False):\n",
    "    # Train with detached embedding targets; early stop on Val(normals) and restore best checkpoint\n",
    "    model.train()\n",
    "\n",
    "    class EarlyStopping:\n",
    "        def __init__(self, patience=3, verbose=False):\n",
    "            self.patience = patience\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = np.inf\n",
    "            self.best_state = None\n",
    "        def step(self, current_score, model):\n",
    "            if current_score < self.best_score - 1e-4: # tolerance of 1e-4\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "                self.best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "                if self.verbose:\n",
    "                    print(\"New best score:\", current_score)\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f\"No improvement. Patience {self.counter}/{self.patience}\")\n",
    "            return self.counter < self.patience\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "\n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    start_time=time.time()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for xb_num, xb_cat, yb_num, yb_cat in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(xb_num, xb_cat)\n",
    "            # detach embedding targets\n",
    "            with torch.no_grad():\n",
    "                true_embeds = []\n",
    "                if xb_cat.shape[1] > 0:\n",
    "                    for i in range(len(model.embeddings)):\n",
    "                        true_embeds.append(model.embeddings[i](yb_cat[:, i]))\n",
    "                y_true_full = torch.cat([yb_num] + true_embeds, dim=1) if true_embeds else yb_num\n",
    "            loss = criterion(y_pred, y_true_full).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        if apply_early_stopping:\n",
    "            if not early_stopping.step(valid_loss, model):\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    training_execution_time=time.time()-start_time\n",
    "    if apply_early_stopping and early_stopping.best_state is not None:\n",
    "        model.load_state_dict(early_stopping.best_state)\n",
    "\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9981a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of numeric features (including one-hot encodings)\n",
    "num_numeric = len(other_cols)\n",
    "\n",
    "# Cardinalities of only high cardinality categorical features\n",
    "cat_cardinalities = [df[col].nunique() for col in high_card_cols]\n",
    "\n",
    "# Initialize the embedding autoencoder model\n",
    "seed_everything(SEED)\n",
    "model = AutoEncoderWithEmbeddings(num_numeric=num_numeric, \n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            hidden1=128, hidden2=64, code_size=8, dropout_rate=0.2)\n",
    "\n",
    "# specify loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Generate losses before training\n",
    "losses = per_sample_mse(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77852422 0.6913051  0.91999686 0.72580612 2.81249809]\n",
      "0.9808297051247598\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65450ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.6593874987803008\n",
      "valid loss: 0.5122092962265015\n",
      "New best score: 0.5122092962265015\n",
      "\n",
      "Epoch 1: train loss: 0.5216837655780907\n",
      "valid loss: 0.48909398913383484\n",
      "New best score: 0.48909398913383484\n",
      "\n",
      "Epoch 2: train loss: 0.4952052961614795\n",
      "valid loss: 0.4779715836048126\n",
      "New best score: 0.4779715836048126\n",
      "\n",
      "Epoch 3: train loss: 0.48269162688936507\n",
      "valid loss: 0.46698975563049316\n",
      "New best score: 0.46698975563049316\n",
      "\n",
      "Epoch 4: train loss: 0.4693346645150866\n",
      "valid loss: 0.45358574390411377\n",
      "New best score: 0.45358574390411377\n",
      "\n",
      "Epoch 5: train loss: 0.4567858086044627\n",
      "valid loss: 0.445102334022522\n",
      "New best score: 0.445102334022522\n",
      "\n",
      "Epoch 6: train loss: 0.4473322482037365\n",
      "valid loss: 0.4362899661064148\n",
      "New best score: 0.4362899661064148\n",
      "\n",
      "Epoch 7: train loss: 0.437493185889452\n",
      "valid loss: 0.4257003664970398\n",
      "New best score: 0.4257003664970398\n",
      "\n",
      "Epoch 8: train loss: 0.42901792140831624\n",
      "valid loss: 0.4185340404510498\n",
      "New best score: 0.4185340404510498\n",
      "\n",
      "Epoch 9: train loss: 0.42211763594383583\n",
      "valid loss: 0.41240379214286804\n",
      "New best score: 0.41240379214286804\n",
      "\n",
      "Epoch 10: train loss: 0.4164194626915724\n",
      "valid loss: 0.40753859281539917\n",
      "New best score: 0.40753859281539917\n",
      "\n",
      "Epoch 11: train loss: 0.4117508289957405\n",
      "valid loss: 0.4022027850151062\n",
      "New best score: 0.4022027850151062\n",
      "\n",
      "Epoch 12: train loss: 0.4063837993862037\n",
      "valid loss: 0.3961999714374542\n",
      "New best score: 0.3961999714374542\n",
      "\n",
      "Epoch 13: train loss: 0.4017708100770649\n",
      "valid loss: 0.39215952157974243\n",
      "New best score: 0.39215952157974243\n",
      "\n",
      "Epoch 14: train loss: 0.39908030732233724\n",
      "valid loss: 0.3904302418231964\n",
      "New best score: 0.3904302418231964\n",
      "\n",
      "Epoch 15: train loss: 0.3967416651266858\n",
      "valid loss: 0.3883630335330963\n",
      "New best score: 0.3883630335330963\n",
      "\n",
      "Epoch 16: train loss: 0.39539456228564557\n",
      "valid loss: 0.386333703994751\n",
      "New best score: 0.386333703994751\n",
      "\n",
      "Epoch 17: train loss: 0.39313986996062716\n",
      "valid loss: 0.38492050766944885\n",
      "New best score: 0.38492050766944885\n",
      "\n",
      "Epoch 18: train loss: 0.39129243945716913\n",
      "valid loss: 0.3832060992717743\n",
      "New best score: 0.3832060992717743\n",
      "\n",
      "Epoch 19: train loss: 0.3891023083288867\n",
      "valid loss: 0.3798030614852905\n",
      "New best score: 0.3798030614852905\n",
      "\n",
      "Epoch 20: train loss: 0.38807114806390347\n",
      "valid loss: 0.3780147433280945\n",
      "New best score: 0.3780147433280945\n",
      "\n",
      "Epoch 21: train loss: 0.38636615670713265\n",
      "valid loss: 0.3750743567943573\n",
      "New best score: 0.3750743567943573\n",
      "\n",
      "Epoch 22: train loss: 0.38402498884308606\n",
      "valid loss: 0.3725491464138031\n",
      "New best score: 0.3725491464138031\n",
      "\n",
      "Epoch 23: train loss: 0.3825009766826056\n",
      "valid loss: 0.3706803321838379\n",
      "New best score: 0.3706803321838379\n",
      "\n",
      "Epoch 24: train loss: 0.38094870255405744\n",
      "valid loss: 0.3685437738895416\n",
      "New best score: 0.3685437738895416\n",
      "\n",
      "Epoch 25: train loss: 0.3795829036630186\n",
      "valid loss: 0.3664546310901642\n",
      "New best score: 0.3664546310901642\n",
      "\n",
      "Epoch 26: train loss: 0.37855831810406276\n",
      "valid loss: 0.36400336027145386\n",
      "New best score: 0.36400336027145386\n",
      "\n",
      "Epoch 27: train loss: 0.3772236818657782\n",
      "valid loss: 0.3621145486831665\n",
      "New best score: 0.3621145486831665\n",
      "\n",
      "Epoch 28: train loss: 0.3754443990556817\n",
      "valid loss: 0.3597263991832733\n",
      "New best score: 0.3597263991832733\n",
      "\n",
      "Epoch 29: train loss: 0.37439456053246234\n",
      "valid loss: 0.35859420895576477\n",
      "New best score: 0.35859420895576477\n",
      "\n",
      "Epoch 30: train loss: 0.3734506125736954\n",
      "valid loss: 0.35735273361206055\n",
      "New best score: 0.35735273361206055\n",
      "\n",
      "Epoch 31: train loss: 0.37235833971123944\n",
      "valid loss: 0.3552081286907196\n",
      "New best score: 0.3552081286907196\n",
      "\n",
      "Epoch 32: train loss: 0.37130448594129173\n",
      "valid loss: 0.3542366325855255\n",
      "New best score: 0.3542366325855255\n",
      "\n",
      "Epoch 33: train loss: 0.3699288109191378\n",
      "valid loss: 0.3532694876194\n",
      "New best score: 0.3532694876194\n",
      "\n",
      "Epoch 34: train loss: 0.3698712894342896\n",
      "valid loss: 0.3532399535179138\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 35: train loss: 0.3684329940860433\n",
      "valid loss: 0.35117968916893005\n",
      "New best score: 0.35117968916893005\n",
      "\n",
      "Epoch 36: train loss: 0.3672433909616972\n",
      "valid loss: 0.3502002954483032\n",
      "New best score: 0.3502002954483032\n",
      "\n",
      "Epoch 37: train loss: 0.3668734435748337\n",
      "valid loss: 0.34930452704429626\n",
      "New best score: 0.34930452704429626\n",
      "\n",
      "Epoch 38: train loss: 0.3662761635350105\n",
      "valid loss: 0.3482823967933655\n",
      "New best score: 0.3482823967933655\n",
      "\n",
      "Epoch 39: train loss: 0.36501793256379605\n",
      "valid loss: 0.34660837054252625\n",
      "New best score: 0.34660837054252625\n",
      "\n",
      "Epoch 40: train loss: 0.36513826676777433\n",
      "valid loss: 0.3459129333496094\n",
      "New best score: 0.3459129333496094\n",
      "\n",
      "Epoch 41: train loss: 0.36396696697500414\n",
      "valid loss: 0.3449695408344269\n",
      "New best score: 0.3449695408344269\n",
      "\n",
      "Epoch 42: train loss: 0.36323833855471216\n",
      "valid loss: 0.3441121578216553\n",
      "New best score: 0.3441121578216553\n",
      "\n",
      "Epoch 43: train loss: 0.36197598379357415\n",
      "valid loss: 0.3435121476650238\n",
      "New best score: 0.3435121476650238\n",
      "\n",
      "Epoch 44: train loss: 0.36140971044848735\n",
      "valid loss: 0.3422495424747467\n",
      "New best score: 0.3422495424747467\n",
      "\n",
      "Epoch 45: train loss: 0.36097825057524485\n",
      "valid loss: 0.3410450518131256\n",
      "New best score: 0.3410450518131256\n",
      "\n",
      "Epoch 46: train loss: 0.3610665053801429\n",
      "valid loss: 0.34081408381462097\n",
      "New best score: 0.34081408381462097\n",
      "\n",
      "Epoch 47: train loss: 0.3601599145204501\n",
      "valid loss: 0.3396936357021332\n",
      "New best score: 0.3396936357021332\n",
      "\n",
      "Epoch 48: train loss: 0.3588144610698958\n",
      "valid loss: 0.3379032611846924\n",
      "New best score: 0.3379032611846924\n",
      "\n",
      "Epoch 49: train loss: 0.35830126073127405\n",
      "valid loss: 0.33704885840415955\n",
      "New best score: 0.33704885840415955\n",
      "\n",
      "Epoch 50: train loss: 0.3586000446986435\n",
      "valid loss: 0.3375491499900818\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 51: train loss: 0.3566608997215902\n",
      "valid loss: 0.33671075105667114\n",
      "New best score: 0.33671075105667114\n",
      "\n",
      "Epoch 52: train loss: 0.3566642467240642\n",
      "valid loss: 0.3341640830039978\n",
      "New best score: 0.3341640830039978\n",
      "\n",
      "Epoch 53: train loss: 0.3561118635468017\n",
      "valid loss: 0.3337595760822296\n",
      "New best score: 0.3337595760822296\n",
      "\n",
      "Epoch 54: train loss: 0.35478932951626024\n",
      "valid loss: 0.33314982056617737\n",
      "New best score: 0.33314982056617737\n",
      "\n",
      "Epoch 55: train loss: 0.35452776976994105\n",
      "valid loss: 0.3327362537384033\n",
      "New best score: 0.3327362537384033\n",
      "\n",
      "Epoch 56: train loss: 0.35334389308341463\n",
      "valid loss: 0.3324529230594635\n",
      "New best score: 0.3324529230594635\n",
      "\n",
      "Epoch 57: train loss: 0.3523936717133773\n",
      "valid loss: 0.3310669958591461\n",
      "New best score: 0.3310669958591461\n",
      "\n",
      "Epoch 58: train loss: 0.3534636959097439\n",
      "valid loss: 0.3293514549732208\n",
      "New best score: 0.3293514549732208\n",
      "\n",
      "Epoch 59: train loss: 0.35108417223270677\n",
      "valid loss: 0.3288658559322357\n",
      "New best score: 0.3288658559322357\n",
      "\n",
      "Epoch 60: train loss: 0.35124109517362784\n",
      "valid loss: 0.3280116021633148\n",
      "New best score: 0.3280116021633148\n",
      "\n",
      "Epoch 61: train loss: 0.349553241182987\n",
      "valid loss: 0.3268849551677704\n",
      "New best score: 0.3268849551677704\n",
      "\n",
      "Epoch 62: train loss: 0.3512792768334984\n",
      "valid loss: 0.3260916769504547\n",
      "New best score: 0.3260916769504547\n",
      "\n",
      "Epoch 63: train loss: 0.3496244972361658\n",
      "valid loss: 0.3258213698863983\n",
      "New best score: 0.3258213698863983\n",
      "\n",
      "Epoch 64: train loss: 0.34897171805675764\n",
      "valid loss: 0.32448700070381165\n",
      "New best score: 0.32448700070381165\n",
      "\n",
      "Epoch 65: train loss: 0.3492630526087338\n",
      "valid loss: 0.3243226110935211\n",
      "New best score: 0.3243226110935211\n",
      "\n",
      "Epoch 66: train loss: 0.3470511535056552\n",
      "valid loss: 0.32313138246536255\n",
      "New best score: 0.32313138246536255\n",
      "\n",
      "Epoch 67: train loss: 0.34812474703430235\n",
      "valid loss: 0.3220731317996979\n",
      "New best score: 0.3220731317996979\n",
      "\n",
      "Epoch 68: train loss: 0.34592786417867905\n",
      "valid loss: 0.32118114829063416\n",
      "New best score: 0.32118114829063416\n",
      "\n",
      "Epoch 69: train loss: 0.346839215612053\n",
      "valid loss: 0.32124611735343933\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 70: train loss: 0.34592121543740867\n",
      "valid loss: 0.31992822885513306\n",
      "New best score: 0.31992822885513306\n",
      "\n",
      "Epoch 71: train loss: 0.34518111258521117\n",
      "valid loss: 0.31928062438964844\n",
      "New best score: 0.31928062438964844\n",
      "\n",
      "Epoch 72: train loss: 0.34549884984367774\n",
      "valid loss: 0.3193868398666382\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 73: train loss: 0.34405689136426254\n",
      "valid loss: 0.3184256851673126\n",
      "New best score: 0.3184256851673126\n",
      "\n",
      "Epoch 74: train loss: 0.34387526104324745\n",
      "valid loss: 0.31773582100868225\n",
      "New best score: 0.31773582100868225\n",
      "\n",
      "Epoch 75: train loss: 0.34369005251647833\n",
      "valid loss: 0.31769728660583496\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 76: train loss: 0.3452337406183544\n",
      "valid loss: 0.3162800371646881\n",
      "New best score: 0.3162800371646881\n",
      "\n",
      "Epoch 77: train loss: 0.3428925305380857\n",
      "valid loss: 0.31572091579437256\n",
      "New best score: 0.31572091579437256\n",
      "\n",
      "Epoch 78: train loss: 0.341684503483593\n",
      "valid loss: 0.31547579169273376\n",
      "New best score: 0.31547579169273376\n",
      "\n",
      "Epoch 79: train loss: 0.3439496216469241\n",
      "valid loss: 0.316114217042923\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 80: train loss: 0.34208577221497555\n",
      "valid loss: 0.3139425814151764\n",
      "New best score: 0.3139425814151764\n",
      "\n",
      "Epoch 81: train loss: 0.34210221148971326\n",
      "valid loss: 0.3140992820262909\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 82: train loss: 0.340995303625451\n",
      "valid loss: 0.31305477023124695\n",
      "New best score: 0.31305477023124695\n",
      "\n",
      "Epoch 83: train loss: 0.34062658974102566\n",
      "valid loss: 0.31212082505226135\n",
      "New best score: 0.31212082505226135\n",
      "\n",
      "Epoch 84: train loss: 0.34125536908780724\n",
      "valid loss: 0.312810480594635\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 85: train loss: 0.3397586018967449\n",
      "valid loss: 0.3114802837371826\n",
      "New best score: 0.3114802837371826\n",
      "\n",
      "Epoch 86: train loss: 0.34017189409499776\n",
      "valid loss: 0.31119000911712646\n",
      "New best score: 0.31119000911712646\n",
      "\n",
      "Epoch 87: train loss: 0.3389562147452419\n",
      "valid loss: 0.3111027479171753\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 88: train loss: 0.33841536520119\n",
      "valid loss: 0.31153327226638794\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 89: train loss: 0.33943559141983665\n",
      "valid loss: 0.31125909090042114\n",
      "No improvement. Patience 3/5\n",
      "\n",
      "Epoch 90: train loss: 0.3388640524301314\n",
      "valid loss: 0.30986276268959045\n",
      "New best score: 0.30986276268959045\n",
      "\n",
      "Epoch 91: train loss: 0.339575406528057\n",
      "valid loss: 0.31052640080451965\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 92: train loss: 0.33770741094323925\n",
      "valid loss: 0.30985915660858154\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 93: train loss: 0.337159157023394\n",
      "valid loss: 0.30921703577041626\n",
      "New best score: 0.30921703577041626\n",
      "\n",
      "Epoch 94: train loss: 0.33662002310717015\n",
      "valid loss: 0.31020617485046387\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 95: train loss: 0.33733624954868974\n",
      "valid loss: 0.3088437616825104\n",
      "New best score: 0.3088437616825104\n",
      "\n",
      "Epoch 96: train loss: 0.3357637835624523\n",
      "valid loss: 0.30863073468208313\n",
      "New best score: 0.30863073468208313\n",
      "\n",
      "Epoch 97: train loss: 0.33564468053050506\n",
      "valid loss: 0.30754274129867554\n",
      "New best score: 0.30754274129867554\n",
      "\n",
      "Epoch 98: train loss: 0.33788094350269865\n",
      "valid loss: 0.3076786398887634\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 99: train loss: 0.3380672469174952\n",
      "valid loss: 0.30782511830329895\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 100: train loss: 0.335715205059912\n",
      "valid loss: 0.30741414427757263\n",
      "New best score: 0.30741414427757263\n",
      "\n",
      "Epoch 101: train loss: 0.33485373012105324\n",
      "valid loss: 0.3071869909763336\n",
      "New best score: 0.3071869909763336\n",
      "\n",
      "Epoch 102: train loss: 0.33692320025056827\n",
      "valid loss: 0.30789121985435486\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 103: train loss: 0.3346692254668788\n",
      "valid loss: 0.3068424463272095\n",
      "New best score: 0.3068424463272095\n",
      "\n",
      "Epoch 104: train loss: 0.33693925473923075\n",
      "valid loss: 0.30647897720336914\n",
      "New best score: 0.30647897720336914\n",
      "\n",
      "Epoch 105: train loss: 0.3334998414928752\n",
      "valid loss: 0.30520501732826233\n",
      "New best score: 0.30520501732826233\n",
      "\n",
      "Epoch 106: train loss: 0.335146755473058\n",
      "valid loss: 0.3051546514034271\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 107: train loss: 0.3354940514367326\n",
      "valid loss: 0.3053966164588928\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 108: train loss: 0.33424861130857825\n",
      "valid loss: 0.3050902187824249\n",
      "New best score: 0.3050902187824249\n",
      "\n",
      "Epoch 109: train loss: 0.3333613525655933\n",
      "valid loss: 0.30538707971572876\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 110: train loss: 0.3332101715238471\n",
      "valid loss: 0.3046630620956421\n",
      "New best score: 0.3046630620956421\n",
      "\n",
      "Epoch 111: train loss: 0.33172492936141507\n",
      "valid loss: 0.30378657579421997\n",
      "New best score: 0.30378657579421997\n",
      "\n",
      "Epoch 112: train loss: 0.33328656106066884\n",
      "valid loss: 0.3036647140979767\n",
      "New best score: 0.3036647140979767\n",
      "\n",
      "Epoch 113: train loss: 0.33084405167658526\n",
      "valid loss: 0.3028571605682373\n",
      "New best score: 0.3028571605682373\n",
      "\n",
      "Epoch 114: train loss: 0.33397019832654107\n",
      "valid loss: 0.3032751679420471\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 115: train loss: 0.33238484725019984\n",
      "valid loss: 0.3034261167049408\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 116: train loss: 0.331093338468021\n",
      "valid loss: 0.30253228545188904\n",
      "New best score: 0.30253228545188904\n",
      "\n",
      "Epoch 117: train loss: 0.3332966210698723\n",
      "valid loss: 0.30207231640815735\n",
      "New best score: 0.30207231640815735\n",
      "\n",
      "Epoch 118: train loss: 0.3313714255067639\n",
      "valid loss: 0.30352360010147095\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 119: train loss: 0.3317421311722662\n",
      "valid loss: 0.3019946813583374\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 120: train loss: 0.33107123321160337\n",
      "valid loss: 0.3017430007457733\n",
      "New best score: 0.3017430007457733\n",
      "\n",
      "Epoch 121: train loss: 0.32997434928004904\n",
      "valid loss: 0.30107539892196655\n",
      "New best score: 0.30107539892196655\n",
      "\n",
      "Epoch 122: train loss: 0.3307425236343441\n",
      "valid loss: 0.30193305015563965\n",
      "No improvement. Patience 1/5\n",
      "\n",
      "Epoch 123: train loss: 0.32968803384250267\n",
      "valid loss: 0.30151018500328064\n",
      "No improvement. Patience 2/5\n",
      "\n",
      "Epoch 124: train loss: 0.32829396626106777\n",
      "valid loss: 0.30234894156455994\n",
      "No improvement. Patience 3/5\n",
      "\n",
      "Epoch 125: train loss: 0.3293607699243646\n",
      "valid loss: 0.3026297390460968\n",
      "No improvement. Patience 4/5\n",
      "\n",
      "Epoch 126: train loss: 0.33017309169123943\n",
      "valid loss: 0.3012019395828247\n",
      "No improvement. Patience 5/5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train with early stopping on Val (normals)\n",
    "model, train_time, _, _ = training_loop(model, train_loader, val_loader, optimizer, criterion,\n",
    "                                        max_epochs=500, apply_early_stopping=True, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57528c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 70th percentile error threshold = 0.29802235960960366\n"
     ]
    }
   ],
   "source": [
    "# Define 70% threshold based on validation set\n",
    "val_errs = per_sample_mse(model, val_loader)\n",
    "thresh = float(np.percentile(val_errs, 70.0))\n",
    "print(\"Validation 70th percentile error threshold =\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b47c19ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC: 0.8035\n"
     ]
    }
   ],
   "source": [
    "# final test evaluation (UID level) using val-based threshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Compute errors on test set\n",
    "test_errs = per_sample_mse(model, test_loader)\n",
    "\n",
    "# Construct dictionary to store UID-level errors\n",
    "uid_errors = defaultdict(list)\n",
    "for uid, err in zip(test_df['UID'].values, test_errs):\n",
    "    uid_errors[uid].append(err)\n",
    "\n",
    "# Average error per UID for average rule\n",
    "uid_avg = {u: float(np.mean(es)) for u, es in uid_errors.items()}\n",
    "\n",
    "# Create data frame to store average error per UID\n",
    "uid_df  = pd.DataFrame.from_dict(uid_avg, orient='index', \n",
    "                                  columns=['avg_error'])\n",
    "\n",
    "# Define ground-truth label\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# metrics\n",
    "auc = roc_auc_score(uid_df['true_label'], uid_df['avg_error'])\n",
    "print(f\"UID‐level ROC AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb69c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 70th percentile val-error threshold = 0.7590\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "uid_df['pred'] = (uid_df['avg_error'] >= thresh).astype(int)\n",
    "recall   = recall_score(uid_df['true_label'], uid_df['pred'])    # binary‐class recall\n",
    "print(f\"Recall at 70th percentile val-error threshold = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7c670b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID‐level ROC AUC (fraction rule):       0.7271\n",
      "UID‐level recall (majority‐vote rule): 0.7641\n"
     ]
    }
   ],
   "source": [
    "# UID‐level evaluation via fraction‐rule\n",
    "\n",
    "# For each UID, compute fraction of its txns flagged as fraud\n",
    "uid_frac = {u: float((np.array(es) >= thresh).mean()) for u, es in uid_errors.items()}\n",
    "\n",
    "# Build DataFrame and true labels\n",
    "uid_df = pd.DataFrame.from_dict(uid_frac, orient='index',\n",
    "                                columns=['fraud_frac'])\n",
    "uid_df['true_label'] = uid_df.index.isin(fraud_uids).astype(int)\n",
    "\n",
    "# Classify UID as fraud if majority of its txns exceed threshold\n",
    "uid_df['pred'] = (uid_df['fraud_frac'] >= 0.50).astype(int)\n",
    "\n",
    "# Compute metrics\n",
    "auc_frac = roc_auc_score(uid_df['true_label'], uid_df['fraud_frac'])\n",
    "recall_frac = recall_score(uid_df['true_label'], uid_df['pred'])\n",
    "\n",
    "print(f\"UID‐level ROC AUC (fraction rule):       {auc_frac:.4f}\")\n",
    "print(f\"UID‐level recall (majority‐vote rule): {recall_frac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
