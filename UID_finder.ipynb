{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72909c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.cluster\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see if we can make UID's by clustering \n",
    "\n",
    "#here we read in data\n",
    "kaggle_set = pd.read_csv('/Users/sara/Desktop/train_transaction.csv')\n",
    "kaggle_id_set = pd.read_csv('/Users/sara/Desktop/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa472d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique identifiers: 43071\n"
     ]
    }
   ],
   "source": [
    "#pull out the categorical card identifiers\n",
    "id_df = kaggle_set[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2']].copy()\n",
    "id_df = id_df.drop_duplicates()\n",
    "\n",
    "#count the number of remaining rows \n",
    "print(\"Number of unique identifiers:\", id_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d962bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's preprocess the data for use of an isolation forest \n",
    "\n",
    "# Drop the labels so they can't be used as a factor\n",
    "df = kaggle_set.copy()\n",
    "\n",
    "# merge in the identity dataset\n",
    "df = df.merge(kaggle_id_set, on=['TransactionID'], how='left')\n",
    "\n",
    "# Sort by transaction date from most recent to oldest\n",
    "df = df.sort_values('TransactionDT',ascending=False)  \n",
    "\n",
    "\n",
    "# Add a count of nans in columns. Not using one-hot encoding because it will cluster fraud cases together. \n",
    "df['nan_count'] = df.isnull().sum(axis=1)\n",
    "\n",
    "\n",
    "# Drop columns that we don't want to tranform so we can use them later\n",
    "df2 = df.drop(columns=['TransactionID', 'TransactionDT', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud']).copy()\n",
    "\n",
    "\n",
    "# Separate into categorical and numerical factors\n",
    "categorical_df = df2.select_dtypes(include=['object']).copy()\n",
    "numerical_df = df2.select_dtypes(exclude=['object']).copy()\n",
    "\n",
    "# Fill missing values in categorical with the term 'missing'\n",
    "categorical_df = categorical_df.fillna('missing')  \n",
    "# use label encoding for categorical columns \n",
    "categorical_df = categorical_df.apply(lambda x: LabelEncoder().fit_transform(x.astype(str)))\n",
    "\n",
    "\n",
    "# Fill missing values in numerical columns with the mean of the column and normalize\n",
    "for column in numerical_df.columns:\n",
    "    # Convert numerical columns to float type and Z-normalize\n",
    "    numerical_df[column] = numerical_df[column].astype(float)\n",
    "    numerical_df[column] = (numerical_df[column]- numerical_df[column].mean())/ numerical_df[column].std()\n",
    "\n",
    "    # Fill missing values with the mean of the column (0 after normalization)\n",
    "    numerical_df[column] = numerical_df[column].fillna(numerical_df[column].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71b230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PCA to reduce dimensionality in numerical data\n",
    "pca = PCA(n_components=0.90)  # Keep 79% of variance (I think this was Adrian's number but correct me if I'm wrong)\n",
    "our_pca = pca.fit_transform(numerical_df.drop(columns=['nan_count']))\n",
    "\n",
    "df_pca = pd.DataFrame(our_pca, columns=[f'PC{i+1}' for i in range(our_pca.shape[1])])\n",
    "\n",
    "# add the categorical, idenfitication, isFraud, and nancount columns back to the PCA dataframe\n",
    "df_pca = pd.concat([df_pca, categorical_df.reset_index(drop=True)], axis = 1)\n",
    "df_pca[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud']] = df[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud']].reset_index(drop=True)\n",
    "df_pca['nan_count'] = numerical_df['nan_count'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6ffe6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in PCA set: ['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15', 'PC16', 'PC17', 'PC18', 'PC19', 'PC20', 'PC21', 'PC22', 'PC23', 'PC24', 'PC25', 'PC26', 'PC27', 'PC28', 'PC29', 'PC30', 'PC31', 'PC32', 'PC33', 'PC34', 'PC35', 'PC36', 'PC37', 'PC38', 'PC39', 'PC40', 'PC41', 'PC42', 'PC43', 'PC44', 'PC45', 'PC46', 'PC47', 'PC48', 'PC49', 'PC50', 'PC51', 'PC52', 'PC53', 'PC54', 'PC55', 'PC56', 'PC57', 'PC58', 'PC59', 'PC60', 'PC61', 'PC62', 'PC63', 'PC64', 'PC65', 'PC66', 'PC67', 'PC68', 'PC69', 'PC70', 'ProductCD', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud', 'nan_count']\n"
     ]
    }
   ],
   "source": [
    "#list columns in PCA set\n",
    "print(\"Columns in PCA set:\", df_pca.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "abc11c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by the card identifiers to reduce the number of similar rows\n",
    "grouped_df_first = df_pca.groupby(['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2']).first().reset_index()\n",
    "grouped_df_max = df_pca.groupby(['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2']).max().reset_index()\n",
    "\n",
    "#drop columns that are only needed for identification and not for clustering\n",
    "grouped_df_final = grouped_df_first.drop(columns=['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee7fc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the Isolation Forest model\n",
    "\n",
    "def detect_anomalies(data, contamination, random_state, n_estimators, max_samples, bootstrap, max_features):\n",
    "#takes a DataFrame, applies Isolation Forest, and returns a DataFrame with anomaly scores and predictions.\n",
    "\n",
    "    # Step 4: Isolation Forest\n",
    "    model = IsolationForest(contamination=contamination, random_state=random_state, n_estimators=n_estimators, max_samples=max_samples, bootstrap=bootstrap, max_features=max_features)\n",
    "    model.fit(data)\n",
    "\n",
    "    scores = model.decision_function(data)\n",
    "    preds = model.predict(data)\n",
    "    scaled_scores = 1 - MinMaxScaler().fit_transform(scores.reshape(-1, 1))\n",
    "\n",
    "    # Step 5: Package results\n",
    "    results = data.copy()\n",
    "    results['anomaly_score'] = scores\n",
    "    results['anomaly_likelihood'] = scaled_scores.flatten()\n",
    "    results['is_anomaly'] = (preds == -1).astype(int)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2d1d0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run isolation forest on the grouped data\n",
    "results = detect_anomalies(grouped_df_final)\n",
    "\n",
    "# add the identifiers and isFraud back to the results\n",
    "results[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'isFraud']] = grouped_df_max[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2','isFraud']]\n",
    "\n",
    "#apply the fraud determination to each group in the original dataset \n",
    "herewego = results[['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'anomaly_score', 'anomaly_likelihood', 'is_anomaly', 'isFraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6cd0e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.1019, Recall: 0.5161, F1 Score: 0.1701\n"
     ]
    }
   ],
   "source": [
    "results['final_ans']= results['isFraud']*4 - results['is_anomaly']\n",
    "\n",
    "TP_count = (results['final_ans'] == 3).sum() # True positive is 4-1 = 3\n",
    "FP_count = (results['final_ans'] == -1).sum() # False positive is 0-1 = -1\n",
    "FN_count = (results['final_ans'] == 4).sum() # False negative is 4-0 = 4\n",
    "TN_count = (results['final_ans'] == 0).sum() # True negative is 0-0 = 0\n",
    "\n",
    "Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {Precision:.4f}, Recall: {Recall:.4f}, F1 Score: {F1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_contamination(data, fraudCompareData ,samplerange=np.arange(250, 2000, 50)):\n",
    "    best_f1 = 0\n",
    "    best_contamination = 0\n",
    "\n",
    "    for contaminations in samplerange:\n",
    "        results = detect_anomalies(data, n_estimators=contaminations)\n",
    "        results['isFraud'] = fraudCompareData['isFraud'] \n",
    "        results['final_ans']= results['isFraud']*4 - results['is_anomaly']\n",
    "        \n",
    "        TP_count = (results['final_ans'] == 3).sum()\n",
    "        FP_count = (results['final_ans'] == -1).sum()\n",
    "        FN_count = (results['final_ans'] == 4).sum()\n",
    "\n",
    "        Precision = TP_count / (TP_count + FP_count) if (TP_count + FP_count) > 0 else 0\n",
    "        Recall = TP_count / (TP_count + FN_count) if (TP_count + FN_count) > 0 else 0\n",
    "        F1_score = 2 * (Precision * Recall) / (Precision + Recall) if (Precision + Recall) > 0 else 0\n",
    "        print(f\"Contamination: {contaminations:.2f}, F1 Score: {F1_score:.4f}, Precision: {Precision:.4f}, Recall: {Recall:.4f}\")\n",
    "\n",
    "        if F1_score > best_f1:\n",
    "            best_f1 = F1_score\n",
    "            best_contamination = contaminations\n",
    "\n",
    "    return best_contamination, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ce66dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contamination: 250.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 300.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 350.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 400.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 450.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 500.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 550.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 600.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 650.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 700.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 750.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 800.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 850.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 900.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 950.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 1000.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 1050.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 1100.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n",
      "Contamination: 1150.00, F1 Score: 0.1701, Precision: 0.1019, Recall: 0.5161\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfind_optimal_contamination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_df_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_df_max\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mfind_optimal_contamination\u001b[39m\u001b[34m(data, fraudCompareData, samplerange)\u001b[39m\n\u001b[32m      3\u001b[39m best_contamination = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m contaminations \u001b[38;5;129;01min\u001b[39;00m samplerange:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     results = \u001b[43mdetect_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontaminations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33misFraud\u001b[39m\u001b[33m'\u001b[39m] = fraudCompareData[\u001b[33m'\u001b[39m\u001b[33misFraud\u001b[39m\u001b[33m'\u001b[39m] \n\u001b[32m      8\u001b[39m     results[\u001b[33m'\u001b[39m\u001b[33mfinal_ans\u001b[39m\u001b[33m'\u001b[39m]= results[\u001b[33m'\u001b[39m\u001b[33misFraud\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m4\u001b[39m - results[\u001b[33m'\u001b[39m\u001b[33mis_anomaly\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mdetect_anomalies\u001b[39m\u001b[34m(data, contamination, random_state, n_estimators, max_samples, bootstrap, max_features)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetect_anomalies\u001b[39m(data, contamination = \u001b[32m.4\u001b[39m, random_state=\u001b[32m11\u001b[39m, n_estimators=\u001b[32m500\u001b[39m, max_samples=\u001b[33m'\u001b[39m\u001b[33m1000\u001b[39m\u001b[33m'\u001b[39m, bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m, max_features=\u001b[32m60.0\u001b[39m):\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#takes a DataFrame, applies Isolation Forest, and returns a DataFrame with anomaly scores and predictions.\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Step 4: Isolation Forest\u001b[39;00m\n\u001b[32m      7\u001b[39m     model = IsolationForest(contamination=contamination, random_state=random_state)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     scores = model.decision_function(data)\n\u001b[32m     11\u001b[39m     preds = model.predict(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_iforest.py:350\u001b[39m, in \u001b[36mIsolationForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.max_samples_ = max_samples\n\u001b[32m    349\u001b[39m max_depth = \u001b[38;5;28mint\u001b[39m(np.ceil(np.log2(\u001b[38;5;28mmax\u001b[39m(max_samples, \u001b[32m2\u001b[39m))))\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[38;5;28mself\u001b[39m._average_path_length_per_tree, \u001b[38;5;28mself\u001b[39m._decision_path_lengths = \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    360\u001b[39m     *[\n\u001b[32m    361\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m     ]\n\u001b[32m    367\u001b[39m )\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.contamination == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    370\u001b[39m     \u001b[38;5;66;03m# 0.5 plays a special role as described in the original paper.\u001b[39;00m\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# we take the opposite as we consider the opposite of their score.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:547\u001b[39m, in \u001b[36mBaseBagging._fit\u001b[39m\u001b[34m(self, X, y, max_samples, max_depth, check_input, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    544\u001b[39m seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n\u001b[32m    545\u001b[39m \u001b[38;5;28mself\u001b[39m._seeds = seeds\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m all_results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ += \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    566\u001b[39m     itertools.chain.from_iterable(t[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[32m    567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:143\u001b[39m, in \u001b[36m_parallel_build_estimators\u001b[39m\u001b[34m(n_estimators, ensemble, X, y, seeds, total_n_estimators, verbose, check_input, fit_params)\u001b[39m\n\u001b[32m    140\u001b[39m     estimator_fit = estimator.fit\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Draw random feature, sample indices\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m features, indices = \u001b[43m_generate_bagging_indices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbootstrap_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m fit_params_ = fit_params.copy()\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# TODO(SLEP6): remove if condition for unrouted sample_weight when metadata\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# routing can't be disabled.\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# 1. If routing is enabled, we will check if the routing supports sample\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# by indexing. The former is more efficient. Therefore, use this method\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# if possible, otherwise use indexing.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:78\u001b[39m, in \u001b[36m_generate_bagging_indices\u001b[39m\u001b[34m(random_state, bootstrap_features, bootstrap_samples, n_features, n_samples, max_features, max_samples)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Randomly draw feature and sample indices.\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Get valid random state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m random_state = \u001b[43mcheck_random_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Draw indices\u001b[39;00m\n\u001b[32m     81\u001b[39m feature_indices = _generate_indices(\n\u001b[32m     82\u001b[39m     random_state, bootstrap_features, n_features, max_features\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1513\u001b[39m, in \u001b[36mcheck_random_state\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.random.mtrand._rand\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, numbers.Integral):\n\u001b[32m-> \u001b[39m\u001b[32m1513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, np.random.RandomState):\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m seed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:185\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/_mt19937.pyx:132\u001b[39m, in \u001b[36mnumpy.random._mt19937.MT19937.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/_core/_ufunc_config.py:485\u001b[39m, in \u001b[36merrstate.__call__.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m _token = _extobj_contextvar.set(extobj)\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    484\u001b[39m     \u001b[38;5;66;03m# Call the original, decorated, function:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    487\u001b[39m     _extobj_contextvar.reset(_token)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "find_optimal_contamination(grouped_df_final, grouped_df_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e9b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
